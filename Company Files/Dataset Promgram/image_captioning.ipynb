{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2s1A9eLRPEj"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:24.893630Z",
     "iopub.status.busy": "2022-12-14T21:23:24.893122Z",
     "iopub.status.idle": "2022-12-14T21:23:24.897127Z",
     "shell.execute_reply": "2022-12-14T21:23:24.896594Z"
    },
    "id": "VRLVEKiTEn04"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFwSaNB8jF7s"
   },
   "source": [
    "&lt;style&gt; td {   text-align: center; }  th {   text-align: center; } &lt;/style&gt;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cffg2i257iMS"
   },
   "source": [
    "# 눈에 띄는 이미지 캡션\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/image_captioning\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서보기</a></td>\n",
    "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/tutorials/text/image_captioning.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행하기</a> </td>\n",
    "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/tutorials/text/image_captioning.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">GitHub에서 소스 보기</a></td>\n",
    "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/tutorials/text/image_captioning.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QASbY_HGo4Lq"
   },
   "source": [
    "아래 예와 같은 이미지가 주어졌을 때의 목표는 \"파도를 타는 서퍼\"와 같은 캡션을 생성하는 것입니다.\n",
    "\n",
    "<table style=\"text-align: center;\">\n",
    "<tr>\n",
    "  <td><img src=\"https://tensorflow.org/images/imcap_prediction.png\" data-md-type=\"image\" alt=\"예측\"></td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <th>서핑하는 남자, 출처: <a href=\"https://commons.wikimedia.org/wiki/Surfing#/media/File:Surfing_in_Hawaii.jpg\">wikimedia</a>\n",
    "</th>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "여기에서 사용된 모델 아키텍처는 [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)의 영감을 받았지만 2단 레이어 트랜스포머 디코더를 사용하도록 업데이트되었습니다. 이 튜토리얼을 최대한 활용하려면 [텍스트 생성](https://www.tensorflow.org/text/tutorials/text_generation),  [seq2seq 모델 및 어텐션](https://www.tensorflow.org/text/tutorials/nmt_with_attention) 또는 [트랜스포머](https://www.tensorflow.org/text/tutorials/transformer)를 약간 경험해 보셔야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HbD8n0w7d3F"
   },
   "source": [
    "이 튜토리얼에서 빌드된 모델 아키텍처는 아래와 같습니다. 특성은 이미지에서 추출되어 트랜스포머 디코더의 크로스 어텐션 레이어로 전달되었습니다.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th>모델 아키텍처</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>    <img width=\"400\" src=\"https://tensorflow.org/images/tutorials/transformer/ImageCaptioning.png\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IxifZKT6vXQ"
   },
   "source": [
    "트랜스포머 디코더는 주로 어텐션 레이어에서 빌드됩니다. 이는 셀프 어텐션을 사용하여 생성되는 시퀀스를 처리하고 크로스 어텐션을 사용하여 이미지를 처리합니다.\n",
    "\n",
    "크로스 어텐션 레이어의 어텐션 가중치를 검사하면 모델이 단어를 생성할 때 이미지의 어떤 부분을 모델이 보고 있는지 알 수 있습니다.\n",
    "\n",
    "![예측](https://tensorflow.org/images/surf.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87us2sLVdwME"
   },
   "source": [
    "이 노트북은 엔드 투 엔드 예제입니다. 노트북을 실행하면 노트북은 데이터세트를 다운로드하며 이미지 특성을 추출하고 캐싱하여 디코더 모델을 훈련합니다. 그런 다음 모델을 사용하여 새로운 이미지에 캡션을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bwwk4uxRz6A"
   },
   "source": [
    "## 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:24.901148Z",
     "iopub.status.busy": "2022-12-14T21:23:24.900672Z",
     "iopub.status.idle": "2022-12-14T21:23:25.064891Z",
     "shell.execute_reply": "2022-12-14T21:23:25.063547Z"
    },
    "id": "gc06pTaBbl72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:25.069456Z",
     "iopub.status.busy": "2022-12-14T21:23:25.069123Z",
     "iopub.status.idle": "2022-12-14T21:23:28.147308Z",
     "shell.execute_reply": "2022-12-14T21:23:28.146133Z"
    },
    "id": "2R1hQGtZEi8Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.10.1\n",
      "Uninstalling tensorflow-2.10.1:\n",
      "  Successfully uninstalled tensorflow-2.10.1\n",
      "Found existing installation: keras 2.10.0\n",
      "Uninstalling keras-2.10.0:\n",
      "  Successfully uninstalled keras-2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages)\n",
      "WARNING: Skipping estimator as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y tensorflow estimator keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:28.152277Z",
     "iopub.status.busy": "2022-12-14T21:23:28.151508Z",
     "iopub.status.idle": "2022-12-14T21:23:51.323422Z",
     "shell.execute_reply": "2022-12-14T21:23:51.322376Z"
    },
    "id": "5Xbt8BkPv8Ou"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_text in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (2.10.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.1-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: tensorflow_datasets in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (4.9.0)\n",
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.9.2-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_text) (0.16.1)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.10.1-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (1.24.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting keras<2.11,>=2.10.0 (from tensorflow)\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: array-record in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (0.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (1.3.0)\n",
      "Requirement already satisfied: promise in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (2.3)\n",
      "INFO: pip is looking at multiple versions of tensorflow-datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.9.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (2.32.2)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (1.13.0)\n",
      "Requirement already satisfied: toml in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (4.66.4)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow_datasets) (6.1.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.6.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow-hub>=0.8.0->tensorflow_text) (2.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.63.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "Using cached tensorflow-2.10.1-cp38-cp38-win_amd64.whl (455.9 MB)\n",
      "Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras, tensorflow\n",
      "Successfully installed keras-2.10.0 tensorflow-2.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.24.4 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.10.1 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.10.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U tensorflow_text tensorflow tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:51.328275Z",
     "iopub.status.busy": "2022-12-14T21:23:51.327730Z",
     "iopub.status.idle": "2022-12-14T21:23:53.161913Z",
     "shell.execute_reply": "2022-12-14T21:23:53.160799Z"
    },
    "id": "7TGZmOuqMia9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages (0.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ6q39Vd-y-7"
   },
   "source": [
    "이 튜토리얼은 주로 데이터세트를 로딩하기 위해 가져오기를 많이 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:53.166303Z",
     "iopub.status.busy": "2022-12-14T21:23:53.166034Z",
     "iopub.status.idle": "2022-12-14T21:23:56.197505Z",
     "shell.execute_reply": "2022-12-14T21:23:56.196785Z"
    },
    "id": "U8l4RJ0XRPEm"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.compat.numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\pandas\\__init__.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\pandas\\compat\\__init__.py:25\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     IS64,\n\u001b[0;32m     19\u001b[0m     PY39,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     PYPY,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     is_numpy_dev,\n\u001b[0;32m     27\u001b[0m     np_version_under1p21,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[0;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[0;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[0;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.compat.numpy'"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "import concurrent.futures\n",
    "import collections\n",
    "import dataclasses\n",
    "import hashlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl9qGnjWrv80"
   },
   "source": [
    "## [선택 사항] 데이터 처리\n",
    "\n",
    "이 섹션은 캡션 데이터세트를 다운로드하고 훈련을 위해 이를 준비합니다. 입력 텍스트를 토큰화하고 사전 훈련된 특정 추출 모델을 통해 모든 이미지를 실행한 결과를 캐싱합니다. 이는 이 섹션의 모든 것을 이해하는 데 중요하지는 않습니다.\n",
    "\n",
    " <section class=\"expandable tfo-display-only-on-site\">\n",
    " <button type=\"button\" class=\"button-red button expand-control\">토글 섹션</button>\n",
    "</section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5e_SigQFiWf"
   },
   "source": [
    "### 데이터세트 선택\n",
    "\n",
    "이 튜토리얼은 데이터세트를 선택할 수 있도록 설정되었습니다. [Flickr8k](https://www.ijcai.org/Proceedings/15/Papers/593.pdf) 또는 [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/) 데이터세트의 작은 슬라이스 중 하나입니다. 이 두 가지는 처음부터 다운로드되고 변환되었지만 [TensorFlow Datasets](https://www.tensorflow.org/datasets)([Coco Captions](https://www.tensorflow.org/datasets/catalog/coco_captions) 및 전체 [Conceptual Captions](https://www.tensorflow.org/datasets/community_catalog/huggingface/conceptual_captions))에서 사용할 수 있는 캡션 데이터세트를 사용하기 위해 튜토리얼을 변환하는 것은 어렵지 않습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqGXX9Dc5c0v"
   },
   "source": [
    "#### Flickr8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:56.202504Z",
     "iopub.status.busy": "2022-12-14T21:23:56.201584Z",
     "iopub.status.idle": "2022-12-14T21:23:56.209106Z",
     "shell.execute_reply": "2022-12-14T21:23:56.208502Z"
    },
    "id": "kaNy_l7tGuAZ"
   },
   "outputs": [],
   "source": [
    "def flickr8k(path='flickr8k'):\n",
    "  path = pathlib.Path(path)\n",
    "\n",
    "  if len(list(path.rglob('*'))) < 16197:\n",
    "    tf.keras.utils.get_file(\n",
    "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
    "        cache_dir='.',\n",
    "        cache_subdir=path,\n",
    "        extract=True)\n",
    "    tf.keras.utils.get_file(\n",
    "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
    "        cache_dir='.',\n",
    "        cache_subdir=path,\n",
    "        extract=True)\n",
    "    \n",
    "  captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
    "  captions = (line.split('\\t') for line in captions)\n",
    "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
    "\n",
    "  cap_dict = collections.defaultdict(list)\n",
    "  for fname, cap in captions:\n",
    "    cap_dict[fname].append(cap)\n",
    "\n",
    "  train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
    "  train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
    "\n",
    "  test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
    "  test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
    "\n",
    "  train_ds = tf.data.experimental.from_list(train_captions)\n",
    "  test_ds = tf.data.experimental.from_list(test_captions)\n",
    "\n",
    "  return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQICBAF4FmSL"
   },
   "source": [
    "#### Conceptual Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:56.212830Z",
     "iopub.status.busy": "2022-12-14T21:23:56.212278Z",
     "iopub.status.idle": "2022-12-14T21:23:56.221793Z",
     "shell.execute_reply": "2022-12-14T21:23:56.221110Z"
    },
    "id": "vQwnxXZXRl12"
   },
   "outputs": [],
   "source": [
    "def conceptual_captions(*, data_dir=\"conceptual_captions\", num_train, num_val):\n",
    "  def iter_index(index_path):\n",
    "    with open(index_path) as f:\n",
    "      for line in f:\n",
    "        caption, url = line.strip().split('\\t')\n",
    "        yield caption, url\n",
    "\n",
    "  def download_image_urls(data_dir, urls):\n",
    "    ex = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
    "    def save_image(url):\n",
    "      hash = hashlib.sha1(url.encode())\n",
    "      # Name the files after the hash of the URL.\n",
    "      file_path = data_dir/f'{hash.hexdigest()}.jpeg'\n",
    "      if file_path.exists():\n",
    "        # Only download each file once.\n",
    "        return file_path\n",
    "\n",
    "      try:\n",
    "        result = requests.get(url, timeout=5)\n",
    "      except Exception:\n",
    "        file_path = None\n",
    "      else:\n",
    "        file_path.write_bytes(result.content)\n",
    "      return file_path\n",
    "    \n",
    "    result = []\n",
    "    out_paths = ex.map(save_image, urls)\n",
    "    for file_path in tqdm.tqdm(out_paths, total=len(urls)):\n",
    "      result.append(file_path)\n",
    "\n",
    "    return result\n",
    "\n",
    "  def ds_from_index_file(index_path, data_dir, count):\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    index = list(itertools.islice(iter_index(index_path), count))\n",
    "    captions = [caption for caption, url in index]\n",
    "    urls = [url for caption, url in index]\n",
    "\n",
    "    paths = download_image_urls(data_dir, urls)\n",
    "\n",
    "    new_captions = []\n",
    "    new_paths = []\n",
    "    for cap, path in zip(captions, paths):\n",
    "      if path is None:\n",
    "        # Download failed, so skip this pair.\n",
    "        continue\n",
    "      new_captions.append(cap)\n",
    "      new_paths.append(path)\n",
    "    \n",
    "    new_paths = [str(p) for p in new_paths]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((new_paths, new_captions))\n",
    "    ds = ds.map(lambda path,cap: (path, cap[tf.newaxis])) # 1 caption per image\n",
    "    return ds\n",
    "\n",
    "  data_dir = pathlib.Path(data_dir)\n",
    "  train_index_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv',\n",
    "    cache_subdir=data_dir,\n",
    "    cache_dir='.')\n",
    "  \n",
    "  val_index_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv',\n",
    "    cache_subdir=data_dir,\n",
    "    cache_dir='.')\n",
    "  \n",
    "  train_raw = ds_from_index_file(train_index_path, data_dir=data_dir/'train', count=num_train)\n",
    "  test_raw = ds_from_index_file(val_index_path, data_dir=data_dir/'val', count=num_val)\n",
    "\n",
    "  return train_raw, test_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> iter_index 함수 </h2> \n",
    "\n",
    "기능\n",
    ">인덱스 파일에서 한 줄씩 읽어와 캡션과 이미지 URL을 추출하는 제너레이터 함수 <br>\n",
    "\n",
    "인자\n",
    ">index_path는 인덱스 파일의 경로를 나타냄 <br>\n",
    "\n",
    "동작\n",
    ">파일을 열고 각 줄에서 탭으로 구분된 캡션과 URL을 추출하여 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> download_image_url 함수 </h2>\n",
    "\n",
    "기능 <br>\n",
    ">이미지 URL 리스트에서 각 URL에 대해 이미지를 다운로드하고 저장하는 함수 <br>\n",
    "\n",
    "인자 <br>\n",
    ">data_dir : 이미지를 저장할 디렉토리 경로 <br>\n",
    ">urls : 다운로드할 이미지 URL 리스트 <br>\n",
    "\n",
    "동작 <br>\n",
    ">ThreadPoolExecutor를 사용하여 병렬로 다운로드를 처리 <br>\n",
    ">각 URL의 해시 값을 파일 이름으로 사용하여 이미지를 저장하고, 이미 존재하는 파일은 건너뜀 <br>\n",
    ">requests.get을 통해 URL에서 이미지 데이터를 가져와 파일로 저장 <br>\n",
    ">다운로드가 실패하면 해당 이미지는 건너뜀 <br>\n",
    ">다운로드 결과인 파일 경로 리스트를 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> ds_from_index_file 함수 </h2> <br> \n",
    "\n",
    "인자 \n",
    ">index_path : 인덱스 파을의 경로 <br>\n",
    ">data_dir : 이미지를 저장할 디렉토리 경로 <br>\n",
    ">count : 가져올 캡션 및 이미지 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동작\n",
    ">iter_index 함수를 사용하여 인덱스 파일에서 캡션과 URL을 추출 <br>\n",
    ">추출한 URL을 이용하여 download_image_urls 함수를 호출하여 이미지를 다운로드하고 저장 <br>\n",
    ">다운로드가 실패한 이미지는 제외하고 유효한 이미지 경로와 캡션만 새로운 리스트에 저장 <br>\n",
    ">TensorFlow Dataset을 생성하고, 이미지 경로와 캡션을 쌍으로 만들어 Dataset을 구성 <br>\n",
    ">각 이미지에 대해 한 개의 캡션만 사용하기 위해 map 함수를 사용하여 캡션을 차원이 추가된 형태로 변환 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBAagBw5p-TM"
   },
   "source": [
    "#### 데이터세트 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFtTZaobquNr"
   },
   "source": [
    "Flickr8k는 이미지당 5개의 캡션과 더욱 소규모의 다운로드를 위한 더 많은 데이터를 포함하고 있어 좋은 선택입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:23:56.225280Z",
     "iopub.status.busy": "2022-12-14T21:23:56.224714Z",
     "iopub.status.idle": "2022-12-14T21:24:34.168022Z",
     "shell.execute_reply": "2022-12-14T21:24:34.167240Z"
    },
    "id": "EJySPbzJ4Wxw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "choose = 'flickr8k'\n",
    "\n",
    "if choose == 'flickr8k':\n",
    "  train_raw, test_raw = flickr8k()\n",
    "else:\n",
    "  train_raw, test_raw = conceptual_captions(num_train=10000, num_val=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UAc275FHxm8"
   },
   "source": [
    "위의 두 데이터세트에 대한 로더는 `(image_path, captions)` 쌍을 포함하는 `tf.data.Dataset`를 반환합니다. Conceptual Captions는 이미지당 캡션 1개를 포함하는 한편 Flickr8k는 이미지당 5개의 캡션을 포함합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:34.172809Z",
     "iopub.status.busy": "2022-12-14T21:24:34.172129Z",
     "iopub.status.idle": "2022-12-14T21:24:34.179431Z",
     "shell.execute_reply": "2022-12-14T21:24:34.178772Z"
    },
    "id": "sAQSps5F8RQI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.element_spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:34.183292Z",
     "iopub.status.busy": "2022-12-14T21:24:34.182753Z",
     "iopub.status.idle": "2022-12-14T21:24:35.874608Z",
     "shell.execute_reply": "2022-12-14T21:24:35.873751Z"
    },
    "id": "xIa0ZaP4tBez"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'flickr8k\\\\Flicker8k_Dataset\\\\2513260012_03d33305cf.jpg', shape=(), dtype=string)\n",
      "tf.Tensor(\n",
      "[b'A black dog is running after a white dog in the snow .'\n",
      " b'Black dog chasing brown dog through snow'\n",
      " b'Two dogs chase each other across the snowy ground .'\n",
      " b'Two dogs play together in the snow .'\n",
      " b'Two dogs running through a low lying body of water .'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for ex_path, ex_captions in train_raw.take(1):\n",
    "  print(ex_path)\n",
    "  print(ex_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cSW4u-ORPFQ"
   },
   "source": [
    "### 이미지 특성 추출기\n",
    "\n",
    "각 이미지에서 특성을 추출하기 위해 이미지 모델(imagenet에서 사전 훈련됨)을 사용할 것입니다. 모델은 이미지 분류기로 훈련되었지만, 설정 `include_top=False`는 최종 분류 레이어 없이 모델을 반환하므로 특성 맵의 최종 레이어를 사용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:35.881246Z",
     "iopub.status.busy": "2022-12-14T21:24:35.880517Z",
     "iopub.status.idle": "2022-12-14T21:24:37.385474Z",
     "shell.execute_reply": "2022-12-14T21:24:37.384654Z"
    },
    "id": "IlUckK8Zfikv"
   },
   "outputs": [],
   "source": [
    "IMAGE_SHAPE=(224, 224, 3)\n",
    "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
    "    input_shape=IMAGE_SHAPE,\n",
    "    include_top=False,\n",
    "    include_preprocessing=True)\n",
    "mobilenet.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dojkiou9gL3R"
   },
   "source": [
    "다음은 모델에 맞게 이미지를 로드하고 크기를 조정하는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:37.389813Z",
     "iopub.status.busy": "2022-12-14T21:24:37.389230Z",
     "iopub.status.idle": "2022-12-14T21:24:37.393233Z",
     "shell.execute_reply": "2022-12-14T21:24:37.392633Z"
    },
    "id": "zXR0217aRPFR"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001E5A977F940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1: football_helmet (0.98)\n",
      "2: ballplayer (0.00)\n",
      "3: shield (0.00)\n",
      "4: breastplate (0.00)\n",
      "5: cuirass (0.00)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# 이미지 파일 경로 설정\n",
    "ex_path = 'C:\\\\Users\\\\kiot\\\\Yolo V5\\\\Flicker8k_Dataset\\\\241347114_6273736da8.jpg'  # 여기에 실제 이미지 파일 경로를 입력하세요\n",
    "\n",
    "# 이미지 로드 및 전처리 함수 정의\n",
    "def load_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return preprocess_input(img_array)\n",
    "\n",
    "# MobileNetV2 모델 로드\n",
    "mobilenet = MobileNetV2(weights='imagenet')\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "test_img = load_image(ex_path)\n",
    "\n",
    "# 배치 차원 추가하지 않고 바로 예측할 이미지\n",
    "predictions = mobilenet.predict(test_img)\n",
    "\n",
    "# 예측 결과 출력\n",
    "decoded_predictions = decode_predictions(predictions, top=5)[0]\n",
    "for i, (imagenet_id, label, score) in enumerate(decoded_predictions):\n",
    "    print(f\"{i + 1}: {label} ({score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JyQ7zS6gzZh"
   },
   "source": [
    "모델은 입력 매치의 각 이미지에 대한 특성 맵을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:37.396884Z",
     "iopub.status.busy": "2022-12-14T21:24:37.396284Z",
     "iopub.status.idle": "2022-12-14T21:24:38.714152Z",
     "shell.execute_reply": "2022-12-14T21:24:38.713367Z"
    },
    "id": "sY86n2i6wJNm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(1, 1000)\n"
     ]
    }
   ],
   "source": [
    "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
    "\n",
    "print(test_img_batch.shape)\n",
    "print(mobilenet(test_img_batch).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyqH3zFwRPFi"
   },
   "source": [
    "### 텍스트 토크나이저/벡터라이저 설정\n",
    "\n",
    "[TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) 레이어를 사용하여 다음 단계에 따라 텍스트 캡션을 정수 시퀀스로 변환하게 됩니다.\n",
    "\n",
    "- [adapt](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization#adapt)를 사용하여 모든 캡션을 반복하고 캡션을 단어로 분할하고 상위 단어의 어휘를 계산합니다.\n",
    "- 각 단어를 어휘의 인덱스에 매핑하여 모든 캡션을 토큰화합니다. 모든 출력 시퀀스는 길이 50으로 채워집니다.\n",
    "- 단어에서 인덱스로, 인덱스에서 단어로의 매핑을 생성하여 결과를 표시합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:38.718028Z",
     "iopub.status.busy": "2022-12-14T21:24:38.717343Z",
     "iopub.status.idle": "2022-12-14T21:24:38.721618Z",
     "shell.execute_reply": "2022-12-14T21:24:38.721001Z"
    },
    "id": "NroZIzB90hD3"
   },
   "outputs": [],
   "source": [
    "def standardize(s):\n",
    "  s = tf.strings.lower(s)\n",
    "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
    "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
    "  return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:38.725034Z",
     "iopub.status.busy": "2022-12-14T21:24:38.724525Z",
     "iopub.status.idle": "2022-12-14T21:24:38.736645Z",
     "shell.execute_reply": "2022-12-14T21:24:38.735945Z"
    },
    "id": "n9SQOXFsyS36"
   },
   "outputs": [],
   "source": [
    "# Use the top 5000 words for a vocabulary.\n",
    "vocabulary_size = 5000\n",
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocabulary_size,\n",
    "    standardize=standardize,\n",
    "    ragged=True)\n",
    "# Learn the vocabulary from the caption data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:38.740148Z",
     "iopub.status.busy": "2022-12-14T21:24:38.739573Z",
     "iopub.status.idle": "2022-12-14T21:24:40.591103Z",
     "shell.execute_reply": "2022-12-14T21:24:40.590193Z"
    },
    "id": "oJGE34aiRPFo"
   },
   "outputs": [],
   "source": [
    "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:40.595502Z",
     "iopub.status.busy": "2022-12-14T21:24:40.594911Z",
     "iopub.status.idle": "2022-12-14T21:24:40.608482Z",
     "shell.execute_reply": "2022-12-14T21:24:40.607807Z"
    },
    "id": "oRahTDtWhJIf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:40.612171Z",
     "iopub.status.busy": "2022-12-14T21:24:40.611576Z",
     "iopub.status.idle": "2022-12-14T21:24:40.649476Z",
     "shell.execute_reply": "2022-12-14T21:24:40.648822Z"
    },
    "id": "-2mGxD33JCxN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[3, 2, 655, 5, 2, 97, 4], [3, 2, 1937, 10, 4]]>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:40.653367Z",
     "iopub.status.busy": "2022-12-14T21:24:40.652809Z",
     "iopub.status.idle": "2022-12-14T21:24:40.739602Z",
     "shell.execute_reply": "2022-12-14T21:24:40.738793Z"
    },
    "id": "8Q44tNQVRPFt"
   },
   "outputs": [],
   "source": [
    "# Create mappings for words to indices and indices to words.\n",
    "word_to_index = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "index_to_word = tf.keras.layers.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:40.744038Z",
     "iopub.status.busy": "2022-12-14T21:24:40.743379Z",
     "iopub.status.idle": "2022-12-14T21:24:40.755471Z",
     "shell.execute_reply": "2022-12-14T21:24:40.754663Z"
    },
    "id": "qo-cfCX3LnHs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[b'[START]', b'a', b'cat', b'in', b'a', b'hat', b'[END]'],\n",
       " [b'[START]', b'a', b'robot', b'dog', b'[END]']]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = index_to_word(t)\n",
    "w.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:40.759207Z",
     "iopub.status.busy": "2022-12-14T21:24:40.758568Z",
     "iopub.status.idle": "2022-12-14T21:24:40.805827Z",
     "shell.execute_reply": "2022-12-14T21:24:40.804968Z"
    },
    "id": "rrUUfGc65vAT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'[START] a cat in a hat [END]', b'[START] a robot dog [END]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEWM9xrYcg45"
   },
   "source": [
    "### 데이터세트 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aX0Z_98S2tN"
   },
   "source": [
    "`train_raw` 및 `test_raw` 데이터세트는 1:많은 `(image, captions)` 쌍을 포함합니다.\n",
    "\n",
    "이 함수는 이미지를 복제하여 캡션에 1:1 이미지가 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:40.809839Z",
     "iopub.status.busy": "2022-12-14T21:24:40.809142Z",
     "iopub.status.idle": "2022-12-14T21:24:40.814840Z",
     "shell.execute_reply": "2022-12-14T21:24:40.814140Z"
    },
    "id": "3_Lqwl9NiGT0"
   },
   "outputs": [],
   "source": [
    "def match_shapes(images, captions):\n",
    "  caption_shape = einops.parse_shape(captions, 'b c')\n",
    "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
    "  images = einops.repeat(\n",
    "      images, 'b ... -> (b c) ...',\n",
    "      c = caption_shape['c'])\n",
    "  return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:40.818304Z",
     "iopub.status.busy": "2022-12-14T21:24:40.817741Z",
     "iopub.status.idle": "2022-12-14T21:24:42.298358Z",
     "shell.execute_reply": "2022-12-14T21:24:42.297445Z"
    },
    "id": "CZGUsuGzUfzt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image paths: (32,)\n",
      "captions: (32, 5)\n",
      "\n",
      "image_paths: (160,)\n",
      "captions: (160,)\n"
     ]
    }
   ],
   "source": [
    "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
    "  break\n",
    "\n",
    "print('image paths:', ex_paths.shape)\n",
    "print('captions:', ex_captions.shape)\n",
    "print()\n",
    "\n",
    "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
    "\n",
    "print('image_paths:', ex_paths.shape)\n",
    "print('captions:', ex_captions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ENR_-swVhnm"
   },
   "source": [
    "keras 훈련과 호환되려면 데이터세트는 `(inputs, labels)` 쌍을 포함해야 합니다. 텍스트 생성의 경우 토큰은 한 단계 이동된 입력과 라벨입니다. 이 함수는 `(images, texts)` 쌍을 `((images, input_tokens), label_tokens)` 쌍으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:42.302644Z",
     "iopub.status.busy": "2022-12-14T21:24:42.302007Z",
     "iopub.status.idle": "2022-12-14T21:24:42.306106Z",
     "shell.execute_reply": "2022-12-14T21:24:42.305450Z"
    },
    "id": "2DsgQ_hZT4C2"
   },
   "outputs": [],
   "source": [
    "def prepare_txt(imgs, txts):\n",
    "  tokens = tokenizer(txts)\n",
    "\n",
    "  input_tokens = tokens[..., :-1]\n",
    "  label_tokens = tokens[..., 1:]\n",
    "  return (imgs, input_tokens), label_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA1x2j0JXX-N"
   },
   "source": [
    "이 함수는 연산을 데이터세트에 추가합니다. 단계는 다음과 같습니다.\n",
    "\n",
    "1. 이미지를 로드합니다(로드에 실패한 이미지는 무시합니다).\n",
    "2. 이미지를 복제하여 캡션의 숫자와 매칭합니다.\n",
    "3. `image, caption` 쌍을 섞고 리배치합니다.\n",
    "4. 텍스트를 토큰화하고 토큰을 이동하여 `label_tokens`을 추가합니다.\n",
    "5. `RaggedTensor` 표현에서 텍스트를 패딩 처리된 밀도 높은 `Tensor` 표현으로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:42.309866Z",
     "iopub.status.busy": "2022-12-14T21:24:42.309306Z",
     "iopub.status.idle": "2022-12-14T21:24:42.314596Z",
     "shell.execute_reply": "2022-12-14T21:24:42.313894Z"
    },
    "id": "4_Pt9zldjQ0q"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .shuffle(10000)\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  def to_tensor(inputs, labels):\n",
    "    (images, in_tok), out_tok = inputs, labels\n",
    "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
    "\n",
    "  return (ds\n",
    "          .map(match_shapes, tf.data.AUTOTUNE)\n",
    "          .unbatch()\n",
    "          .shuffle(shuffle_buffer)\n",
    "          .batch(batch_size)\n",
    "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "          .map(to_tensor, tf.data.AUTOTUNE)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrQ85t1GNfpQ"
   },
   "source": [
    "모델에 특성 추출기를 설치하고 다음과 같이 데이터세트에서 훈련할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:42.318152Z",
     "iopub.status.busy": "2022-12-14T21:24:42.317799Z",
     "iopub.status.idle": "2022-12-14T21:24:43.509852Z",
     "shell.execute_reply": "2022-12-14T21:24:43.509056Z"
    },
    "id": "1KlhOG5cjQ0r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 5), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Assume `load_image` function is defined to load an image from path\n",
    "def load_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    return img_array\n",
    "\n",
    "# Function to prepare dataset\n",
    "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
    "    # Define a function to load image using tf.py_function\n",
    "    def load_image_wrapper(path, caption):\n",
    "        img_tensor = tf.py_function(load_image, [path], tf.float32)\n",
    "        return img_tensor, caption\n",
    "\n",
    "    ds = (ds.shuffle(10000)\n",
    "          .map(load_image_wrapper, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "          .batch(batch_size))\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# Example usage\n",
    "train_ds = prepare_dataset(train_raw, tokenizer)\n",
    "train_ds.element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:43.513817Z",
     "iopub.status.busy": "2022-12-14T21:24:43.513102Z",
     "iopub.status.idle": "2022-12-14T21:24:43.656333Z",
     "shell.execute_reply": "2022-12-14T21:24:43.655548Z"
    },
    "id": "d7Zy9F3zX7i2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None, 5), dtype=tf.string, name=None))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = prepare_dataset(test_raw, tokenizer)\n",
    "test_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZyKygJ8S8zW"
   },
   "source": [
    "### [선택 사항] 이미지 특성 캐싱하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHKhSKhti6NS"
   },
   "source": [
    "이미지 특성 추출기가 변경되지 않으며 이 튜토리얼은 이미지 증강을 사용하지 않으므로 이미지 특성은 캐싱될 수 있습니다. 텍스트 토큰화의 경우도 동일합니다. 캐시를 설정하는 데 드는 시간은 훈련 및 검증 중 각 epoch에서 다시 획득됩니다. 아래의 코드는 두 개의 함수인 `save_dataset` 및 `load_dataset`를 정의합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:43.660482Z",
     "iopub.status.busy": "2022-12-14T21:24:43.659885Z",
     "iopub.status.idle": "2022-12-14T21:24:43.668263Z",
     "shell.execute_reply": "2022-12-14T21:24:43.667576Z"
    },
    "id": "9N1MX5ym6xm5"
   },
   "outputs": [],
   "source": [
    "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
    "  # Load the images and make batches.\n",
    "  ds = (ds\n",
    "        .map(lambda path, caption: (load_image(path), caption))\n",
    "        .apply(tf.data.experimental.ignore_errors())\n",
    "        .batch(batch_size))\n",
    "\n",
    "  # Run the feature extractor on each batch\n",
    "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
    "  def gen():\n",
    "    for (images, captions) in tqdm.tqdm(ds): \n",
    "      feature_maps = image_model(images)\n",
    "\n",
    "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
    "      yield feature_maps, captions\n",
    "\n",
    "  # Wrap the generator in a new tf.data.Dataset.\n",
    "  new_ds = tf.data.Dataset.from_generator(\n",
    "      gen,\n",
    "      output_signature=(\n",
    "          tf.TensorSpec(shape=image_model.output_shape),\n",
    "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
    "\n",
    "  # Apply the tokenization \n",
    "  new_ds = (new_ds\n",
    "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
    "            .unbatch()\n",
    "            .shuffle(1000))\n",
    "\n",
    "  # Save the dataset into shard files.\n",
    "  def shard_func(i, item):\n",
    "    return i % shards\n",
    "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
    "\n",
    "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
    "  def custom_reader_func(datasets):\n",
    "    datasets = datasets.shuffle(1000)\n",
    "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
    "  \n",
    "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
    "\n",
    "  def drop_index(i, x):\n",
    "    return x\n",
    "\n",
    "  ds = (ds\n",
    "        .map(drop_index, tf.data.AUTOTUNE)\n",
    "        .shuffle(shuffle)\n",
    "        .padded_batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:24:43.671624Z",
     "iopub.status.busy": "2022-12-14T21:24:43.671143Z",
     "iopub.status.idle": "2022-12-14T21:25:10.117046Z",
     "shell.execute_reply": "2022-12-14T21:25:10.115984Z"
    },
    "id": "tNdzrenxB3Yy"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (<PIL.Image.Image image mode=RGB size=224x224 at 0x1E5C2F86700>) with an unsupported type (<class 'PIL.Image.Image'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[215], line 41\u001b[0m\n\u001b[0;32m     35\u001b[0m train_raw \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     36\u001b[0m     (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkiot\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mYolo V5\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mFlicker8k_Dataset\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m253762507_9c3356c2f6.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA caption for image 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# 예시 데이터셋 경로\u001b[39;00m\n\u001b[0;32m     38\u001b[0m ]\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# train_raw 데이터셋을 준비하여 ds 변수에 할당\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# tqdm을 사용하여 데이터셋 반복\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (images, captions) \u001b[38;5;129;01min\u001b[39;00m tqdm(ds):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# 이미지와 캡션 처리 로직\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# save_dataset 함수 호출 (train_raw 데이터셋에 대한 처리)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[215], line 17\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[1;34m(dataset, tokenizer)\u001b[0m\n\u001b[0;32m     15\u001b[0m processed_dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path, caption \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m---> 17\u001b[0m     processed_image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     processed_caption \u001b[38;5;241m=\u001b[39m process_caption(caption, tokenizer)  \u001b[38;5;66;03m# process_caption 함수 정의 필요\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     processed_dataset\u001b[38;5;241m.\u001b[39mappend((processed_image, processed_caption))\n",
      "Cell \u001b[1;32mIn[215], line 10\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Ensure the image is in RGB format\u001b[39;00m\n\u001b[0;32m      9\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m))  \u001b[38;5;66;03m# Resize the image\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Convert PIL Image to TensorFlow tensor\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (<PIL.Image.Image image mode=RGB size=224x224 at 0x1E5C2F86700>) with an unsupported type (<class 'PIL.Image.Image'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 이미지 전처리 함수\n",
    "def preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')  # Ensure the image is in RGB format\n",
    "    img = img.resize((224, 224))  # Resize the image\n",
    "    img_array = np.array(img)  # Convert PIL Image to numpy array\n",
    "    img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)  # Convert numpy array to TensorFlow tensor\n",
    "    return img_tensor\n",
    "\n",
    "# 데이터셋 준비 함수\n",
    "def prepare_dataset(dataset, tokenizer):\n",
    "    processed_dataset = []\n",
    "    for image_path, caption in dataset:\n",
    "        processed_image = preprocess_image(image_path)\n",
    "        processed_caption = process_caption(caption, tokenizer)  # process_caption 함수 정의 필요\n",
    "        processed_dataset.append((processed_image, processed_caption))\n",
    "    return processed_dataset\n",
    "\n",
    "# 저장 함수 정의\n",
    "def save_dataset(ds, save_path, image_model, tokenizer):\n",
    "    def gen():\n",
    "        for images, captions in ds:\n",
    "            images = tf.stack([preprocess_image(image) for image in images])\n",
    "            features = image_model(images)\n",
    "            yield features.numpy(), captions.numpy()\n",
    "\n",
    "    for i, (features, captions) in enumerate(gen()):\n",
    "        shard_path = f\"{save_path}_shard_{i}.tfrecord\"\n",
    "        # shard_path에 features와 captions를 저장하는 코드 추가\n",
    "\n",
    "# 예시 데이터셋\n",
    "train_raw = [\n",
    "    (r'C:\\Users\\kiot\\Yolo V5\\Flicker8k_Dataset\\253762507_9c3356c2f6.jpg', 'A caption for image 2')\n",
    "    # 예시 데이터셋 경로\n",
    "]\n",
    "\n",
    "# train_raw 데이터셋을 준비하여 ds 변수에 할당\n",
    "ds = prepare_dataset(train_raw, tokenizer)\n",
    "\n",
    "# tqdm을 사용하여 데이터셋 반복\n",
    "for (images, captions) in tqdm(ds):\n",
    "    # 이미지와 캡션 처리 로직\n",
    "    # save_dataset 함수 호출 (train_raw 데이터셋에 대한 처리)\n",
    "    save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n",
    "\n",
    "# save_dataset 함수 호출 (test_raw 데이터셋에 대한 처리)\n",
    "save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoding of C:\\Users\\kiot\\Yolo V5\\Flickr8k_text\\Flickr_8k.trainImages.txt is: ascii\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "def detect_encoding(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            return result['encoding']\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {file_path}\")\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "# 파일 경로를 수정해서 사용해주세요\n",
    "file_path = r'C:\\Users\\kiot\\Yolo V5\\Flickr8k_text\\Flickr_8k.trainImages.txt'\n",
    "encoding = detect_encoding(file_path)\n",
    "if encoding:\n",
    "    print(f\"The encoding of {file_path} is: {encoding}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[ 0.38823533  0.47450984  0.4901961 ]\n",
      "   [ 0.41176474  0.5058824   0.5058824 ]\n",
      "   [ 0.427451    0.5058824   0.5137255 ]\n",
      "   ...\n",
      "   [ 0.4431373   0.52156866  0.5372549 ]\n",
      "   [ 0.6156863   0.69411767  0.6862745 ]\n",
      "   [ 0.09803927  0.17647064  0.1686275 ]]\n",
      "\n",
      "  [[ 0.4039216   0.4901961   0.5058824 ]\n",
      "   [ 0.41960788  0.5137255   0.5137255 ]\n",
      "   [ 0.45098042  0.5294118   0.5372549 ]\n",
      "   ...\n",
      "   [ 0.24705887  0.28627455  0.30980396]\n",
      "   [ 0.48235297  0.5529412   0.5137255 ]\n",
      "   [ 0.427451    0.47450984  0.47450984]]\n",
      "\n",
      "  [[ 0.41176474  0.5058824   0.5058824 ]\n",
      "   [ 0.41960788  0.5137255   0.5137255 ]\n",
      "   [ 0.45098042  0.5294118   0.5372549 ]\n",
      "   ...\n",
      "   [-0.04313725  0.00392163 -0.02745098]\n",
      "   [ 0.8117647   0.84313726  0.8509804 ]\n",
      "   [-0.6392157  -0.6        -0.64705884]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.05882359  0.05882359 -0.12941176]\n",
      "   [ 0.34901965  0.35686278  0.2941177 ]\n",
      "   [ 0.3176471   0.35686278  0.12156868]\n",
      "   ...\n",
      "   [-0.2235294  -0.29411763 -0.3333333 ]\n",
      "   [-0.24705881 -0.3098039  -0.3333333 ]\n",
      "   [-0.21568626 -0.27843136 -0.30196077]]\n",
      "\n",
      "  [[ 0.20000005  0.21568632  0.12941182]\n",
      "   [ 0.4666667   0.4666667   0.45098042]\n",
      "   [ 0.27058828  0.28627455  0.17647064]\n",
      "   ...\n",
      "   [-0.1607843  -0.19999999 -0.24705881]\n",
      "   [-0.24705881 -0.24705881 -0.3098039 ]\n",
      "   [-0.26274508 -0.26274508 -0.32549018]]\n",
      "\n",
      "  [[ 0.30980396  0.33333337  0.26274514]\n",
      "   [ 0.41176474  0.427451    0.41960788]\n",
      "   [ 0.24705887  0.26274514  0.16078436]\n",
      "   ...\n",
      "   [-0.2862745  -0.31764704 -0.38823527]\n",
      "   [-0.18431371 -0.17647058 -0.2862745 ]\n",
      "   [-0.11372548 -0.12941176 -0.2235294 ]]]\n",
      "\n",
      "\n",
      " [[[-0.41960782 -0.4823529  -0.5686275 ]\n",
      "   [-0.29411763 -0.47450978 -0.58431375]\n",
      "   [-0.20784312 -0.34117645 -0.40392154]\n",
      "   ...\n",
      "   [-0.5137255  -0.44313723 -0.67058825]\n",
      "   [-0.3490196  -0.3098039  -0.56078434]\n",
      "   [-0.81960785 -0.8117647  -0.9529412 ]]\n",
      "\n",
      "  [[-0.69411767 -0.8039216  -0.827451  ]\n",
      "   [-0.6313726  -0.79607844 -0.8039216 ]\n",
      "   [-0.6313726  -0.77254903 -0.88235295]\n",
      "   ...\n",
      "   [-0.78039217 -0.5921569  -0.79607844]\n",
      "   [-0.45098037 -0.27843136 -0.56078434]\n",
      "   [-0.40392154 -0.30196077 -0.4588235 ]]\n",
      "\n",
      "  [[-0.75686276 -0.827451   -0.88235295]\n",
      "   [-0.6156863  -0.8039216  -0.8039216 ]\n",
      "   [-0.6156863  -0.7254902  -0.79607844]\n",
      "   ...\n",
      "   [-0.78039217 -0.70980394 -0.84313726]\n",
      "   [-0.5058824  -0.3960784  -0.6       ]\n",
      "   [-0.7019608  -0.6156863  -0.7411765 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.62352943  0.6392157   0.75686276]\n",
      "   [ 0.5921569   0.6156863   0.7647059 ]\n",
      "   [ 0.5529412   0.6         0.7254902 ]\n",
      "   ...\n",
      "   [ 0.05882359 -0.05098039 -0.4352941 ]\n",
      "   [-0.18431371 -0.20784312 -0.54509807]\n",
      "   [-0.16862744 -0.19999999 -0.5529412 ]]\n",
      "\n",
      "  [[ 0.56078434  0.5764706   0.69411767]\n",
      "   [ 0.5686275   0.5921569   0.7254902 ]\n",
      "   [ 0.5529412   0.6         0.7254902 ]\n",
      "   ...\n",
      "   [-0.35686272 -0.30196077 -0.64705884]\n",
      "   [-0.35686272 -0.27843136 -0.5686275 ]\n",
      "   [-0.47450978 -0.42745095 -0.7254902 ]]\n",
      "\n",
      "  [[ 0.5686275   0.60784316  0.75686276]\n",
      "   [ 0.5529412   0.54509807  0.6862745 ]\n",
      "   [ 0.5921569   0.58431375  0.69411767]\n",
      "   ...\n",
      "   [-0.52156866 -0.34117645 -0.67058825]\n",
      "   [-0.21568626 -0.16862744 -0.45098037]\n",
      "   [-0.24705881 -0.27843136 -0.5058824 ]]]\n",
      "\n",
      "\n",
      " [[[-0.2235294  -0.3098039  -0.45098037]\n",
      "   [-0.24705881 -0.3098039  -0.4588235 ]\n",
      "   [-0.25490195 -0.34117645 -0.4823529 ]\n",
      "   ...\n",
      "   [-0.12156862 -0.23921567 -0.40392154]\n",
      "   [-0.06666666 -0.18431371 -0.36470586]\n",
      "   [-0.0745098  -0.19215685 -0.372549  ]]\n",
      "\n",
      "  [[-0.23137254 -0.29411763 -0.44313723]\n",
      "   [-0.23921567 -0.30196077 -0.45098037]\n",
      "   [-0.26274508 -0.32549018 -0.47450978]\n",
      "   ...\n",
      "   [-0.10588235 -0.2235294  -0.372549  ]\n",
      "   [-0.06666666 -0.18431371 -0.3490196 ]\n",
      "   [-0.06666666 -0.18431371 -0.3490196 ]]\n",
      "\n",
      "  [[-0.23921567 -0.29411763 -0.44313723]\n",
      "   [-0.24705881 -0.30196077 -0.45098037]\n",
      "   [-0.25490195 -0.31764704 -0.46666664]\n",
      "   ...\n",
      "   [-0.09019607 -0.20784312 -0.35686272]\n",
      "   [-0.05882353 -0.17647058 -0.32549018]\n",
      "   [-0.05882353 -0.17647058 -0.32549018]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.23137254 -0.38039213 -0.7176471 ]\n",
      "   [-0.23921567 -0.372549   -0.70980394]\n",
      "   [-0.26274508 -0.3960784  -0.73333335]\n",
      "   ...\n",
      "   [-0.46666664 -0.54509807 -0.827451  ]\n",
      "   [-0.47450978 -0.5764706  -0.8509804 ]\n",
      "   [-0.4823529  -0.58431375 -0.85882354]]\n",
      "\n",
      "  [[-0.25490195 -0.3960784  -0.75686276]\n",
      "   [-0.27058822 -0.40392154 -0.7490196 ]\n",
      "   [-0.29411763 -0.42745095 -0.7647059 ]\n",
      "   ...\n",
      "   [-0.4823529  -0.5529412  -0.8745098 ]\n",
      "   [-0.4980392  -0.5921569  -0.8901961 ]\n",
      "   [-0.4980392  -0.5921569  -0.8901961 ]]\n",
      "\n",
      "  [[-0.10588235 -0.27843136 -0.6       ]\n",
      "   [-0.15294117 -0.29411763 -0.60784316]\n",
      "   [-0.18431371 -0.32549018 -0.62352943]\n",
      "   ...\n",
      "   [-0.4980392  -0.5921569  -0.8901961 ]\n",
      "   [-0.4823529  -0.5921569  -0.8980392 ]\n",
      "   [-0.47450978 -0.58431375 -0.8901961 ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[-0.9607843  -0.9372549  -0.99215686]\n",
      "   [-0.9372549  -0.84313726 -0.9529412 ]\n",
      "   [-0.85882354 -0.78039217 -0.96862745]\n",
      "   ...\n",
      "   [-0.15294117 -0.17647058 -0.3098039 ]\n",
      "   [ 0.05098045  0.00392163 -0.12156862]\n",
      "   [ 0.12156868  0.07450986 -0.05098039]]\n",
      "\n",
      "  [[-0.827451   -0.8117647  -0.9843137 ]\n",
      "   [-0.6627451  -0.6        -1.        ]\n",
      "   [-0.6156863  -0.54509807 -0.9607843 ]\n",
      "   ...\n",
      "   [-0.20784312 -0.19999999 -0.32549018]\n",
      "   [-0.10588235 -0.12941176 -0.24705881]\n",
      "   [-0.00392157 -0.02745098 -0.14509803]]\n",
      "\n",
      "  [[-0.78039217 -0.78039217 -0.96862745]\n",
      "   [-0.654902   -0.6        -1.        ]\n",
      "   [-0.49019605 -0.45098037 -0.90588236]\n",
      "   ...\n",
      "   [-0.2235294  -0.2235294  -0.30196077]\n",
      "   [-0.20784312 -0.19215685 -0.30196077]\n",
      "   [-0.12941176 -0.11372548 -0.21568626]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 0.41960788  0.34901965  0.37254906]\n",
      "   [ 0.3176471   0.2313726   0.26274514]\n",
      "   [ 0.30980396  0.21568632  0.2313726 ]\n",
      "   ...\n",
      "   [ 0.48235297  0.35686278  0.16078436]\n",
      "   [ 0.4431373   0.3411765   0.19215691]\n",
      "   [ 0.45882356  0.36470592  0.19215691]]\n",
      "\n",
      "  [[ 0.32549024  0.27843142  0.2941177 ]\n",
      "   [ 0.35686278  0.30980396  0.32549024]\n",
      "   [ 0.27058828  0.20000005  0.22352946]\n",
      "   ...\n",
      "   [ 0.4039216   0.3176471   0.06666672]\n",
      "   [ 0.38823533  0.26274514  0.05882359]\n",
      "   [ 0.5058824   0.37254906  0.15294123]]\n",
      "\n",
      "  [[ 0.21568632  0.1686275   0.18431377]\n",
      "   [ 0.21568632  0.1686275   0.18431377]\n",
      "   [ 0.22352946  0.2313726   0.26274514]\n",
      "   ...\n",
      "   [ 0.27843142  0.20784318 -0.03529412]\n",
      "   [ 0.2313726   0.18431377 -0.06666666]\n",
      "   [ 0.19215691  0.1686275  -0.09019607]]]\n",
      "\n",
      "\n",
      " [[[-0.12941176 -0.12941176 -0.2235294 ]\n",
      "   [-0.1607843  -0.15294117 -0.26274508]\n",
      "   [-0.17647058 -0.17647058 -0.27058822]\n",
      "   ...\n",
      "   [ 0.05098045  0.05098045 -0.02745098]\n",
      "   [ 0.082353    0.082353    0.00392163]\n",
      "   [ 0.07450986  0.07450986 -0.00392157]]\n",
      "\n",
      "  [[-0.09803921 -0.09019607 -0.19999999]\n",
      "   [-0.09803921 -0.09019607 -0.19999999]\n",
      "   [-0.10588235 -0.10588235 -0.19999999]\n",
      "   ...\n",
      "   [ 0.2313726   0.2313726   0.15294123]\n",
      "   [ 0.26274514  0.26274514  0.18431377]\n",
      "   [ 0.26274514  0.26274514  0.18431377]]\n",
      "\n",
      "  [[-0.12156862 -0.11372548 -0.2235294 ]\n",
      "   [-0.10588235 -0.09803921 -0.20784312]\n",
      "   [-0.11372548 -0.11372548 -0.20784312]\n",
      "   ...\n",
      "   [ 0.28627455  0.28627455  0.20784318]\n",
      "   [ 0.30196083  0.30196083  0.22352946]\n",
      "   [ 0.30196083  0.30196083  0.22352946]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.3490196  -0.17647058 -0.7490196 ]\n",
      "   [-0.4352941  -0.27058822 -0.79607844]\n",
      "   [-0.25490195 -0.08235294 -0.5921569 ]\n",
      "   ...\n",
      "   [-0.2235294  -0.05882353 -0.6       ]\n",
      "   [-0.19215685 -0.03529412 -0.6       ]\n",
      "   [-0.09803921  0.07450986 -0.4980392 ]]\n",
      "\n",
      "  [[-0.27843136 -0.11372548 -0.654902  ]\n",
      "   [-0.40392154 -0.23921567 -0.7647059 ]\n",
      "   [-0.3960784  -0.2235294  -0.73333335]\n",
      "   ...\n",
      "   [-0.20784312 -0.01960784 -0.6       ]\n",
      "   [-0.19215685 -0.05882353 -0.6156863 ]\n",
      "   [-0.05098039  0.10588241 -0.4588235 ]]\n",
      "\n",
      "  [[-0.4980392  -0.3333333  -0.84313726]\n",
      "   [-0.36470586 -0.19999999 -0.7254902 ]\n",
      "   [-0.3490196  -0.16862744 -0.7019608 ]\n",
      "   ...\n",
      "   [-0.19999999 -0.03529412 -0.6392157 ]\n",
      "   [-0.12941176  0.02745104 -0.5372549 ]\n",
      "   [-0.23921567 -0.08235294 -0.64705884]]]\n",
      "\n",
      "\n",
      " [[[-0.20784312 -0.05098039 -0.06666666]\n",
      "   [-0.19999999 -0.04313725 -0.05882353]\n",
      "   [-0.19999999 -0.04313725 -0.05882353]\n",
      "   ...\n",
      "   [ 0.9607843   0.9529412   0.92156863]\n",
      "   [ 0.96862745  0.9607843   0.92941177]\n",
      "   [ 0.96862745  0.9607843   0.92941177]]\n",
      "\n",
      "  [[-0.18431371 -0.03529412 -0.05098039]\n",
      "   [-0.17647058 -0.01960784 -0.03529412]\n",
      "   [-0.18431371 -0.02745098 -0.04313725]\n",
      "   ...\n",
      "   [ 0.9764706   0.96862745  0.9372549 ]\n",
      "   [ 0.96862745  0.9607843   0.92941177]\n",
      "   [ 0.9607843   0.9529412   0.92156863]]\n",
      "\n",
      "  [[-0.17647058 -0.01176471 -0.05098039]\n",
      "   [-0.16862744 -0.00392157 -0.04313725]\n",
      "   [-0.16862744 -0.01176471 -0.02745098]\n",
      "   ...\n",
      "   [ 0.9764706   0.96862745  0.9372549 ]\n",
      "   [ 0.96862745  0.9607843   0.92941177]\n",
      "   [ 0.96862745  0.9607843   0.92941177]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-1.         -1.         -1.        ]\n",
      "   [-1.         -1.         -1.        ]\n",
      "   [-0.99215686 -1.         -1.        ]\n",
      "   ...\n",
      "   [-0.8980392  -0.96862745 -0.94509804]\n",
      "   [-0.96862745 -1.         -1.        ]\n",
      "   [-0.47450978 -0.56078434 -0.73333335]]\n",
      "\n",
      "  [[-0.99215686 -1.         -1.        ]\n",
      "   [-0.9764706  -0.9843137  -1.        ]\n",
      "   [-0.9764706  -1.         -1.        ]\n",
      "   ...\n",
      "   [-0.8352941  -0.9372549  -1.        ]\n",
      "   [-0.5294118  -0.654902   -0.75686276]\n",
      "   [-0.56078434 -0.6784314  -0.84313726]]\n",
      "\n",
      "  [[-0.9607843  -0.99215686 -1.        ]\n",
      "   [-0.9137255  -0.92156863 -0.9529412 ]\n",
      "   [-0.96862745 -1.         -0.99215686]\n",
      "   ...\n",
      "   [-0.58431375 -0.69411767 -0.79607844]\n",
      "   [-0.827451   -0.9529412  -1.        ]\n",
      "   [-0.60784316 -0.69411767 -0.8509804 ]]]], shape=(32, 224, 224, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[b'Two guys running along a wall .'\n",
      "  b'Two men holding their arms out from their bodies .'\n",
      "  b'Two men in red shirts appear to be walking on the exterior walls of a decorative building .'\n",
      "  b'Two men in red shirts walking on the walls of a white building .'\n",
      "  b'Two men wearing dark pants and red shirts are leaping in-synch in front of a white building .']\n",
      " [b'One white boy and one black boy stand with arms around each other .'\n",
      "  b'Two boys , one black and one white , standing arm in arm .'\n",
      "  b'Two boys put their arms around each other and pose .'\n",
      "  b'Two young boys , one white and one black , are standing with their arms around each other .'\n",
      "  b'Two young boys with their arms around one another .']\n",
      " [b'A dog in water holds a stick in its mouth .'\n",
      "  b'A German Shepherd is running with a stick in its mouth .'\n",
      "  b'A large yellow dog is retrieving a stick .'\n",
      "  b'a tan dog holding a stick in his mouth .'\n",
      "  b'The yellow dog is carrying a stick by water .']\n",
      " [b'A man in a yellow helmet climbs up a rock cliff .'\n",
      "  b'a man wearing a yellow helmet climbing the side of a large rock wall .'\n",
      "  b'A man wearing a yellow helmet tries to climb a rock with his hands .'\n",
      "  b'A mountain climber reaches the top of a cliff .'\n",
      "  b'A person wearing a yellow helmet is climbing a cliff .']\n",
      " [b'A boy jumps a skateboard while a man in a suit looks on .'\n",
      "  b'A man walks past as a skateboarder flies over the steps .'\n",
      "  b'a skateboarder jumps over a set of stairs .'\n",
      "  b'A skateboarder performs a trick .'\n",
      "  b'person on a skateboard in the air']\n",
      " [b'A black dog leaps in the air while playing outside .'\n",
      "  b'A small black and white dog jumps with red plastic fence in background .'\n",
      "  b'A small black dog in midair .'\n",
      "  b'a small black dog jumps over a bar .'\n",
      "  b'A small , black dog jumps through the air while a person beckons it .']\n",
      " [b'A black and white dog stands on the grass with a baseball in his mouth'\n",
      "  b'A closeup of a small black and white dog with a baseball in its mouth .'\n",
      "  b'Black and white dog holding a baseball in mouth .'\n",
      "  b'Brown and white dog with baseball in mouth , field and building .'\n",
      "  b'brown and white dog with a baseball in its mouth .']\n",
      " [b'Two girls in backless dresses .'\n",
      "  b'Two girls in evening wear are looking back and smiling .'\n",
      "  b'Two pretty ladies show off the low cut backs of their dresses'\n",
      "  b'Two smiling women in backless dresses facing away from the camera have their heads turned to look behind them .'\n",
      "  b'Two women wearing bare-back dresses look over their shoulder for a picture .']\n",
      " [b'A group of children are sitting on a raft wearing blue helmets and carrying paddles .'\n",
      "  b'A group of kids paddle a raft made of sticks and rope .'\n",
      "  b'Children paddle on a craft .'\n",
      "  b'Children wearing life preserves and helmets , and holding an oar , sit on a raft in water .'\n",
      "  b'several young boys wearing blue helmets and blue and red life jackets on a raft']\n",
      " [b'A skateboarder in midair while doing a jump .'\n",
      "  b'A skateboarder is airborne between a white building and an American flag .'\n",
      "  b'a young skateboarder jumping a trick in the air .'\n",
      "  b'Man jumping with a skateboard in a parking lot .'\n",
      "  b'The skateboarder is jumping in the air .']\n",
      " [b'A dog catches a yellow Frisbee thrown by a gray-haired man .'\n",
      "  b'A man is watching a dog catch a Frisbee in midair .'\n",
      "  b'A man stands by as a dog leaps to grab a Frisbee while people sitting on a wall watch .'\n",
      "  b'A man tossing a Frisbee to his dog that leaps to catch it .'\n",
      "  b'Dog jumps to catch a Frisbee while people watch .']\n",
      " [b'a man jumping in a mud puddle in the middle of the street'\n",
      "  b'A man jumps up and down in a puddle in a parking lot .'\n",
      "  b'A smiling person wearing a jacket and boots jumps in a big puddle .'\n",
      "  b'Man jumping for joy in a rain storm at the beach .'\n",
      "  b'Person in blue coat jumping in standing water on pavement with body of water in the background .']\n",
      " [b'A child goes skiing .' b'A young girl skiing alongside an adult'\n",
      "  b'A young skier receives a lesson in the snow from an adult .'\n",
      "  b'Man and child skiing .'\n",
      "  b\"The ski instructor is teaching the little girl how to bend her knees and stand on her ski 's .\"]\n",
      " [b'A basketball player holds the ball during a game .'\n",
      "  b'Man with a basketball in a Florida uniform .'\n",
      "  b'The basketball player in white is holding an orange basketball .'\n",
      "  b'The basketball player is wearing a Florida jersey .'\n",
      "  b'There is a Florida college player dribbling the ball .']\n",
      " [b'Three women dance before a flag .' b'Three women dancing in robes .'\n",
      "  b'Three women in dresses dancing .'\n",
      "  b'Three women in saris dance near a large flag .'\n",
      "  b'Three women wearing saris are dancing in front of a flag that is hanging on a wall .']\n",
      " [b'A brown dog wades into a lake to retrieve a stick .'\n",
      "  b'A dog carries a very big stick through the water .'\n",
      "  b'A dog holds a stick in its mouth in the water .'\n",
      "  b'A dog is running through a creek with a stick in its mouth .'\n",
      "  b'A reddish brown dog walking in water carrying a stick in his mouth .']\n",
      " [b'A group of people stand at a farmers market on a dreary day .'\n",
      "  b'a white tented fruit stand with several people shopping in it .'\n",
      "  b'People shop for fresh produce at an outdoor market in the city .'\n",
      "  b'People visiting a street market .'\n",
      "  b\"Several people shop at an outdoor farmer 's market on a cloudy day .\"]\n",
      " [b'A man in a blue jersey is kicking a ball while another man watches .'\n",
      "  b'A man in a number ten jersey kicks a soccer ball while another team member watches .'\n",
      "  b'A rugby player swings his leg back to punt the ball .'\n",
      "  b'Number ten is kicking the ball as number seven watches .'\n",
      "  b'Two men in blue jersey tops and white shorts play soccer ,']\n",
      " [b'A boy in a red shirt walks down the street through heavy rain .'\n",
      "  b'A boy in yellow walks down the street .'\n",
      "  b'A boy walks in the rain with tennis balls in his shirt .'\n",
      "  b'A child carring something in their shirt walks in the rain on the street .'\n",
      "  b\"A girl holding a bag walks down the street in front of the chief justice 's office .\"]\n",
      " [b'A military man says hello to a group of children waving American flags .'\n",
      "  b'a veteran in a brown uniform at a parade with children and American flags'\n",
      "  b'Military official greets children who are waving American flags .'\n",
      "  b'The man in the military uniform greets children waving American flags .'\n",
      "  b'Young children waving American flags for a soldier']\n",
      " [b'The children play in the pool .'\n",
      "  b'Two children are playing in an outdoor swimming pool .'\n",
      "  b'Two children playing in a swimming pool .'\n",
      "  b'Two children play in the water of an above-ground pool .'\n",
      "  b'Two little girls playing in the pool .']\n",
      " [b'A black dog is running through the snow .'\n",
      "  b'A black Labrador bounds across the snow .'\n",
      "  b'A little black dog is running on the snow .'\n",
      "  b'A small black dog runs through the white snow .'\n",
      "  b'The black dog is running through the snow .']\n",
      " [b'A woman , covered in a red and blue shawl , smoking a cigarette , eyes closed .'\n",
      "  b'A woman with a black sweatband is wrapped in a blue , red and yellow blanket and is smoking .'\n",
      "  b'A woman with a headband wearing a blue , yellow and red hooded jacket .'\n",
      "  b'The woman with a colorful blanket is smoking a pipe .'\n",
      "  b'The woman wrapped a flag around her body and burned incense .']\n",
      " [b'A brown dog is running on the grass .'\n",
      "  b'A brown dog is running very fast with his tongue out .'\n",
      "  b'A brown dog running through the grass .'\n",
      "  b'A brown dog runs on the grass .'\n",
      "  b'A brown dog runs through the grass .']\n",
      " [b'A man falling off his surfboard on the top of a wave .'\n",
      "  b'A man is tumbling into the water having fallen off a surfboard .'\n",
      "  b'A person is falling off their surfboard .'\n",
      "  b'A person performing watersports gets seperated from his apparatus .'\n",
      "  b'A person wearing a red shirt is falling off a white surfboard .']\n",
      " [b'A dog is jumping over a log in a wooded area while carrying another log .'\n",
      "  b'A dog with a stick in his mouth jumps over a fallen tree in the forest .'\n",
      "  b'Dog carries stick and jumps over a log .'\n",
      "  b'The dog carries a stick and jumps over a log in the woods .'\n",
      "  b'The dog jumps over the log with a stick in its mouth .']\n",
      " [b'A boy is airborne on his skateboard above a set of rails in an industrial setting .'\n",
      "  b'A boy with a skateboard if jumping in the air along some railway tracks .'\n",
      "  b'A man doing a skateboard trick over a railroad track .'\n",
      "  b'A skateboarder does a kickflip along the train tracks .'\n",
      "  b'A skateboarder doing a jump on train tracks .']\n",
      " [b'A group of hikers led by a black and white dog climbs a hillside trail covered in ferns .'\n",
      "  b'A group of three people and one dog hike up a green mountainside .'\n",
      "  b'People with a dog hike up a green mountain .'\n",
      "  b'Three hikers and a dog are walking through the foliage on the hillside .'\n",
      "  b'Three people hiking up a mountain with a river and other mountains in the background .']\n",
      " [b'A biker gets high in the air against a skyline .'\n",
      "  b'A boy wearing a white helmet jumping on his bike .'\n",
      "  b'a cyclist is performing a jumping stunt in front of a city skyline .'\n",
      "  b'A man is in the air on his bicycle .'\n",
      "  b'A person doing a bicycle jump , a skyline in the background .']\n",
      " [b'A little girl on a bouncey playground animal sits while a woman takes her picture .'\n",
      "  b'A woman and a child .'\n",
      "  b'A woman in a checked coat is taking a photograph of a little girl sitting on a fake tiger .'\n",
      "  b'A woman stands next to a child who is riding on a toy .'\n",
      "  b'The lady takes a picture of the little girl on the ride .']\n",
      " [b'A man in a purple jersey is falling down while chasing a player in a green jersey playing soccer'\n",
      "  b'A man in green kick a soccer ball while a man in purple and white is falling down .'\n",
      "  b'The players manuever for the soccer ball .'\n",
      "  b'Two men kick at a soccer ball .'\n",
      "  b'Two men wearing different colored uniforms playing soccer .']\n",
      " [b'A dirt bike racer takes offf as others watch .'\n",
      "  b'A motocross rider is traveling along a dirt path with people watching .'\n",
      "  b'A person on a dirt bike is riding up a hill while people watch on the side .'\n",
      "  b'A person riding a dirt bike .'\n",
      "  b'Person riding dirt bike on track while others stand by track']], shape=(32, 5), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for (images, captions) in ds.take(1):\n",
    "    print(images)\n",
    "    print(captions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "798DtfH51UI8"
   },
   "source": [
    " </section>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI265LiDslr2"
   },
   "source": [
    "## 훈련을 위한 데이터 준비\n",
    "\n",
    "이러한 사전 처리 단계 후, 데이터세트는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.121323Z",
     "iopub.status.busy": "2022-12-14T21:25:10.120772Z",
     "iopub.status.idle": "2022-12-14T21:25:10.228921Z",
     "shell.execute_reply": "2022-12-14T21:25:10.228170Z"
    },
    "id": "Pwic2YCjHZmV"
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc1 in position 72: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[206], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_cache\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_cache\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 40\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(save_path, batch_size, shuffle, cycle_length)\u001b[0m\n\u001b[0;32m     37\u001b[0m   datasets \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39minterleave(\u001b[38;5;28;01mlambda\u001b[39;00m x: x, cycle_length\u001b[38;5;241m=\u001b[39mcycle_length)\n\u001b[1;32m---> 40\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_reader_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop_index\u001b[39m(i, x):\n\u001b[0;32m     43\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1835\u001b[0m, in \u001b[0;36mDatasetV2.load\u001b[1;34m(path, element_spec, compression, reader_func)\u001b[0m\n\u001b[0;32m   1832\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency\u001b[39;00m\n\u001b[0;32m   1833\u001b[0m \u001b[38;5;66;03m# dataset_ops->load_ops->dataset_ops\u001b[39;00m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m-> 1835\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1836\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43melement_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melement_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreader_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\tensorflow\\python\\data\\ops\\load_op.py:36\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, element_spec, compression, reader_func)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(path,\n\u001b[0;32m     33\u001b[0m          element_spec,\n\u001b[0;32m     34\u001b[0m          compression,\n\u001b[0;32m     35\u001b[0m          reader_func):\n\u001b[1;32m---> 36\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_LoadDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melement_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\tensorflow\\python\\data\\ops\\load_op.py:57\u001b[0m, in \u001b[0;36m_LoadDataset.__init__\u001b[1;34m(self, path, element_spec, compression, reader_func)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     54\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn graph mode the `element_spec` argument must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gfile\u001b[38;5;241m.\u001b[39mGFile(\n\u001b[0;32m     56\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, dataset_ops\u001b[38;5;241m.\u001b[39mDATASET_SPEC_FILENAME), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 57\u001b[0m   encoded_spec \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m struct_pb \u001b[38;5;241m=\u001b[39m nested_structure_coder\u001b[38;5;241m.\u001b[39mstruct_pb2\u001b[38;5;241m.\u001b[39mStructuredValue()\n\u001b[0;32m     59\u001b[0m struct_pb\u001b[38;5;241m.\u001b[39mParseFromString(encoded_spec)\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:114\u001b[0m, in \u001b[0;36mFileIO.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the contents of a file as a string.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m  Starts reading from current position in file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    string if in string (regular) mode.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preread_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    116\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:76\u001b[0m, in \u001b[0;36mFileIO._preread_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_check_passed:\n\u001b[0;32m     74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mPermissionDeniedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     75\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open for reading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_buf \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBufferedInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc1 in position 72: invalid start byte"
     ]
    }
   ],
   "source": [
    "train_ds = load_dataset('train_cache')\n",
    "test_ds = load_dataset('test_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.233359Z",
     "iopub.status.busy": "2022-12-14T21:25:10.232823Z",
     "iopub.status.idle": "2022-12-14T21:25:10.237794Z",
     "shell.execute_reply": "2022-12-14T21:25:10.236872Z"
    },
    "id": "3B80JXj7HloX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(None, 7, 7, 576), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
       " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jfb8qknlsKi"
   },
   "source": [
    "데이터세트는 이제 keras 훈련에 적합한 `(input, label)` 쌍을 반환합니다. `inputs`은 `(images, input_tokens)` 쌍입니다. `images`는 특성-추출기 모델로 처리됩니다. `input_tokens`의 각 위치의 경우 모델은 지금까지의 텍스트를 보고 `labels`의 같은 위치에서 나열된 다음 텍스트를 예측하려고 시도합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.241704Z",
     "iopub.status.busy": "2022-12-14T21:25:10.241167Z",
     "iopub.status.idle": "2022-12-14T21:25:10.457216Z",
     "shell.execute_reply": "2022-12-14T21:25:10.456446Z"
    },
    "id": "YJBEwuXLZQdw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (inputs, ex_labels) \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m   (ex_img, ex_in_tok) \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mex_img\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ex_in_tok\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(ex_labels\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ex_img' is not defined"
     ]
    }
   ],
   "source": [
    "for (inputs, ex_labels) in train_ds.take(1):\n",
    "  (ex_img, ex_in_tok) = inputs\n",
    "\n",
    "print(ex_img.shape)\n",
    "print(ex_in_tok.shape)\n",
    "print(ex_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22R58DzZoF17"
   },
   "source": [
    "입력 토큰 및 라벨은 동일하며, 다음과 같이 한 단계만 이동하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.461129Z",
     "iopub.status.busy": "2022-12-14T21:25:10.460617Z",
     "iopub.status.idle": "2022-12-14T21:25:10.466555Z",
     "shell.execute_reply": "2022-12-14T21:25:10.465877Z"
    },
    "id": "V7h5UGftn1hT"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex_in_tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[165], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mex_in_tok\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ex_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ex_in_tok' is not defined"
     ]
    }
   ],
   "source": [
    "print(ex_in_tok[0].numpy())\n",
    "print(ex_labels[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfICM49WFpIb"
   },
   "source": [
    "## 트랜스포머 디코더 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONyjuWsmZoyO"
   },
   "source": [
    "이 모델은 사전 훈련된 이미지 인코더가 충분하다고 가정하며 텍스트 디코더를 빌드하는 데만 집중합니다. 이 튜토리얼은 2단 레이어 트렌스포머 디코더를 사용합니다.\n",
    "\n",
    "이 구현은 [Transformers 튜토리얼](https://www.tensorflow.org/text/tutorials/transformer)의 구현과 거의 동일합니다. 더 자세한 내용은 이를 다시 참조하세요.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th>트랜스포머 인코더 및 디코더.</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>    <img width=\"400\" src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\">\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiRXWwIKNybB"
   },
   "source": [
    "모델은 다음과 같은 세 가지 주요 부분으로 구현됩니다.\n",
    "\n",
    "1. 입력 - 토큰 임베딩 및 위치 인코딩(`SeqEmbedding`).\n",
    "2. 디코더 - 각각 다음을 포함하는 트랜스포머 디코더 레이어(`DecoderLayer`)의 스택\n",
    "    1. 추후에 각 출력 위치가 지금까지 출력에 대해 처리할 수 있는 인과적 셀프 어텐션(`CausalSelfAttention`).\n",
    "    2. 각 출력 위치가 입력 이미지를 추리할 수 있는 크로스 어텐션 레이어(`CrossAttention`).\n",
    "    3. 각 출력 위치를 독립적으로 추가로 처리하는 피드 포워드 네트워크(`FeedForward`) 레이어.\n",
    "3. 출력 - 출력 어휘에 대한 멀티 클래스 분류.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ngm3SQMCaYU"
   },
   "source": [
    "### 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9suaARZGPKw"
   },
   "source": [
    "입력 텍스트는 이미 토큰으로 분할되고 ID 시퀀스로 변환되었습니다.\n",
    "\n",
    "CNN 또는 RNN와는 다르게 트랜스포머의 어텐션 레이어는 시퀀스의 순서에 대해 변하지 않는다는 점을 기억하세요. 몇몇 위치 입력이 없다면 시퀀스가 아닌 순서 없는 세트만 봅니다. 따라서 각 토큰 ID에 대한 단순한 벡터 임베딩 외에도 임베딩 레이어는 시퀀스 내 각 위치에 대한 임베딩 또한 포함합니다.\n",
    "\n",
    "`SeqEmbedding` 레이어는 다음과 같이 정의됩니다.\n",
    "\n",
    "- 각 토큰에 대한 임베딩 벡터를 검색합니다.\n",
    "- 각 시퀀스 위치에 대한 임베딩 벡터를 검색합니다.\n",
    "- 두 개를 모두 합합니다.\n",
    "- `mask_zero=True`를 사용하여 모델에 대한 keras 마스크를 초기화합니다.\n",
    "\n",
    "참고: 이 구현은 [Transformer 튜토리얼](https://www.tensorflow.org/text/tutorials/transformer)에서와 같이 고정된 임베딩을 사용하는 대신 위치 임베딩을 학습합니다. 임베딩을 학습하는 것은 코드가 약간 적지만 더 긴 시퀀스로 일반화되지는 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.470365Z",
     "iopub.status.busy": "2022-12-14T21:25:10.469849Z",
     "iopub.status.idle": "2022-12-14T21:25:10.475338Z",
     "shell.execute_reply": "2022-12-14T21:25:10.474703Z"
    },
    "id": "P91LU2F0a9Ga"
   },
   "outputs": [],
   "source": [
    "class SeqEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, max_length, depth):\n",
    "    super().__init__()\n",
    "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
    "\n",
    "    self.token_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=depth,\n",
    "        mask_zero=True)\n",
    "    \n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, seq):\n",
    "    seq = self.token_embedding(seq) # (batch, seq, depth)\n",
    "\n",
    "    x = tf.range(tf.shape(seq)[1])  # (seq)\n",
    "    x = x[tf.newaxis, :]  # (1, seq)\n",
    "    x = self.pos_embedding(x)  # (1, seq, depth)\n",
    "\n",
    "    return self.add([seq,x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II1mD-bBCdMB"
   },
   "source": [
    "### 디코더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHMLeMtKPTCW"
   },
   "source": [
    "디코더는 표준 트랜스포머 디코더로, 각 세 개의 하위 레이어인 `CausalSelfAttention`, `CrossAttention` 및 `FeedForward`를 포함하는 `DecoderLayers`의 스택을 포함합니다. 구현은 [Transformer 튜토리얼](https://www.tensorflow.org/text/tutorials/transformer)과 거의 동일하며, 자세한 내용은 이를 참조하세요.\n",
    "\n",
    "다음은 `CausalSelfAttention` 레이어입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.479038Z",
     "iopub.status.busy": "2022-12-14T21:25:10.478455Z",
     "iopub.status.idle": "2022-12-14T21:25:10.483250Z",
     "shell.execute_reply": "2022-12-14T21:25:10.482612Z"
    },
    "id": "6JTLiX3lKooQ"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    # Use Add instead of + so the keras mask propagates through.\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    attn = self.mha(query=x, value=x,\n",
    "                    use_causal_mask=True)\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c66OTRwQfd8"
   },
   "source": [
    "아래는 `CrossAttention` 레이어입니다. `return_attention_scores`를 사용하는 데 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.486741Z",
     "iopub.status.busy": "2022-12-14T21:25:10.486202Z",
     "iopub.status.idle": "2022-12-14T21:25:10.491371Z",
     "shell.execute_reply": "2022-12-14T21:25:10.490714Z"
    },
    "id": "rIY6Vu2pLBAO"
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self,**kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.add = tf.keras.layers.Add() \n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x, y, **kwargs):\n",
    "    attn, attention_scores = self.mha(\n",
    "             query=x, value=y,\n",
    "             return_attention_scores=True)\n",
    "    \n",
    "    self.last_attention_scores = attention_scores\n",
    "\n",
    "    x = self.add([x, attn])\n",
    "    return self.layernorm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hn5p6f-RE0C"
   },
   "source": [
    "아래는 `FeedForward` 레이어입니다. `layers.Dense` 레이어는 입력의 최종 축에 적용된다는 점을 기억하세요. 입력의 형태는 `(batch, sequence, channels)`이므로 `batch` 및 `sequence` 축에 걸쳐 포인트별로 자동으로 적용됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.494690Z",
     "iopub.status.busy": "2022-12-14T21:25:10.494224Z",
     "iopub.status.idle": "2022-12-14T21:25:10.499156Z",
     "shell.execute_reply": "2022-12-14T21:25:10.498524Z"
    },
    "id": "cWKrl7teOnH2"
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=units),\n",
    "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "    ])\n",
    "\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = x + self.seq(x)\n",
    "    return self.layernorm(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbXoiVNPRoJc"
   },
   "source": [
    "다음으로 이러한 세 가지 레이어를 더 큰 규모의 `DecoderLayer`에 배열합니다. 각 디코더 레이어는 시퀀스에 세 개의 더 작은 레이어를 적용합니다. 각 하위 레이어 다음의 `out_seq` 형태는 `(batch, sequence, channels)`입니다. 디코더 레이어는 또한 추후 시각화를 위한 `attention_scores`를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.502479Z",
     "iopub.status.busy": "2022-12-14T21:25:10.501895Z",
     "iopub.status.idle": "2022-12-14T21:25:10.507305Z",
     "shell.execute_reply": "2022-12-14T21:25:10.506638Z"
    },
    "id": "ydcW5KZZHou7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
    "                                              key_dim=units,\n",
    "                                              dropout=dropout_rate)\n",
    "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
    "                                          key_dim=units,\n",
    "                                          dropout=dropout_rate)\n",
    "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
    "      \n",
    "\n",
    "  def call(self, inputs, training=False):\n",
    "    in_seq, out_seq = inputs\n",
    "\n",
    "    # Text input\n",
    "    out_seq = self.self_attention(out_seq)\n",
    "\n",
    "    out_seq = self.cross_attention(out_seq, in_seq)\n",
    "    \n",
    "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
    "\n",
    "    out_seq = self.ff(out_seq)\n",
    "\n",
    "    return out_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lgbYrF5Csqu"
   },
   "source": [
    "### 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcnKZkrklAQf"
   },
   "source": [
    "출력 레이어는 각 위치에서 각 토큰에 대한 로짓 예측을 생성하려면 최소한 `layers.Dense` 레이어가 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WQD87efena5"
   },
   "source": [
    "하지만 이 작업을 좀 더 잘 수행할 수 있도록 추가할 수 있는 몇 가지 다른 특성이 있습니다.\n",
    "\n",
    "1. **잘못된 토큰 처리**: 모델은 텍스트를 생성합니다. 패드, 알 수 없는, 또는 시작 토큰(`''`, `'[UNK]'`, `'[START]'`)을 생성해서는 안됩니다. 따라서 이들에 대한 편향을 큰 음수 값으로 설정합니다.\n",
    "\n",
    "    > 참고: 손실 함수의 이러한 토큰 역시 무시해야 합니다.\n",
    "\n",
    "2. **스마트 초기화**: 밀도가 높은 레이어의 기본 초기화는 거의 균일한 확률로 각 토큰을 초기에 예측하는 모델을 제공합니다. 실제 토큰 분포는 균일한 것과는 거리가 멉니다. 출력 레이어의 초기 편향을 위한 최적값은 각 토큰의 확률 로그입니다. 따라서 `adapt` 메서드를 포함해 토큰의 수를 세고 최적의 초기 편향을 설정합니다. 이는 균일한 분포(`log(vocabulary_size)`)의 엔트로피로부터의 분포의 한계 엔트로피(`-p*log(p)`)로 초기 손실을 줄입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.510629Z",
     "iopub.status.busy": "2022-12-14T21:25:10.510111Z",
     "iopub.status.idle": "2022-12-14T21:25:10.517944Z",
     "shell.execute_reply": "2022-12-14T21:25:10.517168Z"
    },
    "id": "CeWw2SFDHUfo"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class TokenOutput(tf.keras.layers.Layer):\n",
    "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(\n",
    "        units=tokenizer.vocabulary_size(), **kwargs)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.banned_tokens = banned_tokens\n",
    "\n",
    "    self.bias = None\n",
    "\n",
    "  def adapt(self, ds):\n",
    "    counts = collections.Counter()\n",
    "    vocab_dict = {name: id \n",
    "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
    "\n",
    "    for tokens in tqdm.tqdm(ds):\n",
    "      counts.update(tokens.numpy().flatten())\n",
    "\n",
    "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
    "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
    "\n",
    "    counts_arr = counts_arr[:]\n",
    "    for token in self.banned_tokens:\n",
    "      counts_arr[vocab_dict[token]] = 0\n",
    "\n",
    "    total = counts_arr.sum()\n",
    "    p = counts_arr/total\n",
    "    p[counts_arr==0] = 1.0\n",
    "    log_p = np.log(p)  # log(1) == 0\n",
    "\n",
    "    entropy = -(log_p*p).sum()\n",
    "\n",
    "    print()\n",
    "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
    "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
    "\n",
    "    self.bias = log_p\n",
    "    self.bias[counts_arr==0] = -1e9\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense(x)\n",
    "    # TODO(b/250038731): Fix this.\n",
    "    # An Add layer doesn't work because of the different shapes.\n",
    "    # This clears the mask, that's okay because it prevents keras from rescaling\n",
    "    # the losses.\n",
    "    return x + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzQHqANd1A6Q"
   },
   "source": [
    "스마트 초기화는 초기 손실을 다음과 같이 상당히 줄입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:10.521168Z",
     "iopub.status.busy": "2022-12-14T21:25:10.520594Z",
     "iopub.status.idle": "2022-12-14T21:25:13.418916Z",
     "shell.execute_reply": "2022-12-14T21:25:13.417963Z"
    },
    "id": "GGnOQyc501B2"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'tqdm' has no attribute 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m output_layer \u001b[38;5;241m=\u001b[39m TokenOutput(tokenizer, banned_tokens\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# This might run a little faster if the dataset didn't also have to load the image data.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43moutput_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[75], line 18\u001b[0m, in \u001b[0;36mTokenOutput.adapt\u001b[1;34m(self, ds)\u001b[0m\n\u001b[0;32m     14\u001b[0m counts \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mCounter()\n\u001b[0;32m     15\u001b[0m vocab_dict \u001b[38;5;241m=\u001b[39m {name: \u001b[38;5;28mid\u001b[39m \n\u001b[0;32m     16\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_vocabulary())}\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtqdm\u001b[49m(ds):\n\u001b[0;32m     19\u001b[0m   counts\u001b[38;5;241m.\u001b[39mupdate(tokens\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     21\u001b[0m counts_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocabulary_size(),))\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'tqdm' has no attribute 'tqdm'"
     ]
    }
   ],
   "source": [
    "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
    "# This might run a little faster if the dataset didn't also have to load the image data.\n",
    "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gq-ICN7bD-u"
   },
   "source": [
    "### 모델 빌드하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gou4fPH_SWgH"
   },
   "source": [
    "모델을 빌드하려면 몇몇 부분을 조합해야 합니다.\n",
    "\n",
    "1. 이미지 `feature_extractor` 및 텍스트 `tokenizer`.\n",
    "2. 토큰 ID의 배치를 벡터 `(batch, sequence, channels)`로 변환하기 위한 `seq_embedding` 레이어.\n",
    "3. 텍스트 및 이미지 데이터를 처리할 `DecoderLayers` 레이어의 스택.\n",
    "4. 다음 단어가 무엇이어야 하는지에 대한 포인트별 예측을 반환하는 `output_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:13.423406Z",
     "iopub.status.busy": "2022-12-14T21:25:13.422677Z",
     "iopub.status.idle": "2022-12-14T21:25:13.429585Z",
     "shell.execute_reply": "2022-12-14T21:25:13.428750Z"
    },
    "id": "bHCISYehH1f6"
   },
   "outputs": [],
   "source": [
    "class Captioner(tf.keras.Model):\n",
    "  @classmethod\n",
    "  def add_method(cls, fun):\n",
    "    setattr(cls, fun.__name__, fun)\n",
    "    return fun\n",
    "\n",
    "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
    "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.feature_extractor = feature_extractor\n",
    "    self.tokenizer = tokenizer\n",
    "    self.word_to_index = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary())\n",
    "    self.index_to_word = tf.keras.layers.StringLookup(\n",
    "        mask_token=\"\",\n",
    "        vocabulary=tokenizer.get_vocabulary(),\n",
    "        invert=True) \n",
    "\n",
    "    self.seq_embedding = SeqEmbedding(\n",
    "        vocab_size=tokenizer.vocabulary_size(),\n",
    "        depth=units,\n",
    "        max_length=max_length)\n",
    "\n",
    "    self.decoder_layers = [\n",
    "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        for n in range(num_layers)]\n",
    "\n",
    "    self.output_layer = output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YW390dOz9T-x"
   },
   "source": [
    "훈련을 위해 모델을 호출하면 `image, txt` 쌍을 수신합니다. 이 함수를 더욱 유용하게 하려면 입력에 대해 더 유연해지세요.\n",
    "\n",
    "- 이미지에 3개의 채널이 있다면 feature_extractor를 통해 실행합니다. 그렇지 않으면 이미 실행된 것으로 가정합니다.\n",
    "- 텍스트에 dtype `tf.string`이 있다면 토크나이저를 통해 실행하세요.\n",
    "\n",
    "그런 다음 모델을 실행하는 것은 몇 단계만 수행하면 됩니다.\n",
    "\n",
    "1. 추출된 이미지 특성을 평면화하여 디코더 레이어에 대한 입력이 될 수 있도록 합니다.\n",
    "2. 토큰 임베딩을 검색합니다.\n",
    "3. 이미지 특성 및 텍스트 임베딩에서 `DecoderLayer`의 스택을 실행합니다.\n",
    "4. 출력 레이어를 실행하여 각 위치에서 다음 토큰을 예측합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:13.433193Z",
     "iopub.status.busy": "2022-12-14T21:25:13.432682Z",
     "iopub.status.idle": "2022-12-14T21:25:13.437834Z",
     "shell.execute_reply": "2022-12-14T21:25:13.436979Z"
    },
    "id": "lPdb7I4h9Ulo"
   },
   "outputs": [],
   "source": [
    "  @Captioner.add_method\n",
    "  def call(self, inputs):\n",
    "    image, txt = inputs\n",
    "\n",
    "    if image.shape[-1] == 3:\n",
    "      # Apply the feature-extractor, if you get an RGB image.\n",
    "      image = self.feature_extractor(image)\n",
    "    \n",
    "    # Flatten the feature map\n",
    "    image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
    "\n",
    "\n",
    "    if txt.dtype == tf.string:\n",
    "      # Apply the tokenizer if you get string inputs.\n",
    "      txt = tokenizer(txt)\n",
    "\n",
    "    txt = self.seq_embedding(txt)\n",
    "\n",
    "    # Look at the image\n",
    "    for dec_layer in self.decoder_layers:\n",
    "      txt = dec_layer(inputs=(image, txt))\n",
    "      \n",
    "    txt = self.output_layer(txt)\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:13.441433Z",
     "iopub.status.busy": "2022-12-14T21:25:13.440927Z",
     "iopub.status.idle": "2022-12-14T21:25:13.556524Z",
     "shell.execute_reply": "2022-12-14T21:25:13.555764Z"
    },
    "id": "kmM7aZQsLiyU"
   },
   "outputs": [],
   "source": [
    "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
    "                  units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGvOcLQKghXN"
   },
   "source": [
    "### 캡션 생성하기\n",
    "\n",
    "훈련을 시작하기 전에, 코드를 약간 작성해 캡션을 생성합니다. 이를 사용하여 훈련이 어떻게 진행되는지 확인합니다.\n",
    "\n",
    "다음과 같이 테스트 이미지를 다운로드하여 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:13.560884Z",
     "iopub.status.busy": "2022-12-14T21:25:13.560324Z",
     "iopub.status.idle": "2022-12-14T21:25:13.986758Z",
     "shell.execute_reply": "2022-12-14T21:25:13.985890Z"
    },
    "id": "cwFcdMqC-jE2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://tensorflow.org/images/surf.jpg\n",
      "64400/64400 [==============================] - 0s 3us/step\n"
     ]
    }
   ],
   "source": [
    "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
    "image = load_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRBIiTkubmxA"
   },
   "source": [
    "이 모델로 이미지를 캡션하려면 다음을 수행합니다.\n",
    "\n",
    "- `img_features` 추출\n",
    "- `[START]` 토큰으로 출력 토큰 목록 초기화.\n",
    "- `img_features` 및 `tokens`를 모델로 전달.\n",
    "    - 이는 로짓 목록을 반환합니다.\n",
    "    - 이러한 로짓을 기반으로 다음 토큰을 선택합니다.\n",
    "    - 토큰 목록에 이를 추가하고 루프를 계속합니다.\n",
    "    - `'[END]'` 토큰이 생성되었다면 루프를 벗어나세요.\n",
    "\n",
    "이를 위해 \"간단한\" 메서드를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:13.990970Z",
     "iopub.status.busy": "2022-12-14T21:25:13.990245Z",
     "iopub.status.idle": "2022-12-14T21:25:13.997395Z",
     "shell.execute_reply": "2022-12-14T21:25:13.996502Z"
    },
    "id": "Nf1Jie9ef_Cg"
   },
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def simple_gen(self, image, temperature=1):\n",
    "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
    "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
    "\n",
    "  tokens = initial # (batch, sequence)\n",
    "  for n in range(50):\n",
    "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
    "    preds = preds[:,-1, :]  #(batch, vocab)\n",
    "    if temperature==0:\n",
    "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
    "    else:\n",
    "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
    "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence) \n",
    "\n",
    "    if next[0] == self.word_to_index('[END]'):\n",
    "      break\n",
    "  words = index_to_word(tokens[0, 1:-1])\n",
    "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
    "  return result.numpy().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxN2NPX2zB8y"
   },
   "source": [
    "다음은 모델의 훈련되지 않은, 해당 이미지를 위해 생성된 일부 캡션으로 아직 의미가 그다지 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:14.001078Z",
     "iopub.status.busy": "2022-12-14T21:25:14.000475Z",
     "iopub.status.idle": "2022-12-14T21:25:15.909817Z",
     "shell.execute_reply": "2022-12-14T21:25:15.908795Z"
    },
    "id": "sPm96CccvHnq"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[1;32mIn[81], line 4\u001b[0m, in \u001b[0;36msimple_gen\u001b[1;34m(self, image, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@Captioner\u001b[39m\u001b[38;5;241m.\u001b[39madd_method\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_gen\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m   initial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   img_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m   tokens \u001b[38;5;241m=\u001b[39m initial \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for t in (0.0, 0.5, 1.0):\n",
    "  result = model.simple_gen(image, temperature=t)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JefwCRZ8z-Ah"
   },
   "source": [
    "온도 매개변수를 통해 다음 세 모드 사이에 삽입할 수 있습니다.\n",
    "\n",
    "1. 그리디 디코딩(`temperature=0.0`) - 각 단계에서 가장 확률이 높은 다음 토큰을 선택합니다.\n",
    "2. 로짓(`temperature=1.0`)에 따른 랜덤 샘플링.\n",
    "3. 균일 랜덤 샘플링(`temperature >> 1.0`).\n",
    "\n",
    "모델이 훈련되지 않았고 빈도 기반 초기화를 사용하였으므로 \"그리디\" 출력은 (우선) 일반적으로 가장 일반적인 토큰인 `['a', '.', '[END]']`만 포함합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0FpTvaPkqON"
   },
   "source": [
    "## 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKcwZdqObK-U"
   },
   "source": [
    "모델을 훈련하려면 다음과 같은 몇몇 추가 컴포넌트가 필요합니다.\n",
    "\n",
    "- 손실 및 메트릭\n",
    "- 옵티마이저\n",
    "- 선택적 콜백"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5IW2mWa2sAG"
   },
   "source": [
    "### 손실 및 메트릭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbpbDQTw1lOW"
   },
   "source": [
    "다음은 마스킹 된 손실 및 정확성에 대한 구현입니다.\n",
    "\n",
    "손실에 대한 마스크를 계산할 때 `loss < 1e8`를 주의하세요. 이 항은 `banned_tokens`에 대한 인공적이고 불가능할 정도로 높은 손실을 버립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:15.914604Z",
     "iopub.status.busy": "2022-12-14T21:25:15.914051Z",
     "iopub.status.idle": "2022-12-14T21:25:15.920265Z",
     "shell.execute_reply": "2022-12-14T21:25:15.919410Z"
    },
    "id": "s24im3FqxAfT"
   },
   "outputs": [],
   "source": [
    "def masked_loss(labels, preds):  \n",
    "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
    "\n",
    "  mask = (labels != 0) & (loss < 1e8) \n",
    "  mask = tf.cast(mask, loss.dtype)\n",
    "\n",
    "  loss = loss*mask\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "def masked_acc(labels, preds):\n",
    "  mask = tf.cast(labels!=0, tf.float32)\n",
    "  preds = tf.argmax(preds, axis=-1)\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  match = tf.cast(preds == labels, mask.dtype)\n",
    "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOhjHqgv3F2e"
   },
   "source": [
    "### 콜백"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dyQN9UfJYEd"
   },
   "source": [
    "훈련 중 피드백을 위해 `keras.callbacks.Callback`을 설정해 각 epoch의 끝에 서퍼 이미지에 대한 일부 캡션을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:15.924456Z",
     "iopub.status.busy": "2022-12-14T21:25:15.923866Z",
     "iopub.status.idle": "2022-12-14T21:25:15.928981Z",
     "shell.execute_reply": "2022-12-14T21:25:15.928113Z"
    },
    "id": "IKDwbZOCZ-AP"
   },
   "outputs": [],
   "source": [
    "class GenerateText(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
    "    self.image = load_image(image_path)\n",
    "\n",
    "  def on_epoch_end(self, epochs=None, logs=None):\n",
    "    print()\n",
    "    print()\n",
    "    for t in (0.0, 0.5, 1.0):\n",
    "      result = self.model.simple_gen(self.image, temperature=t)\n",
    "      print(result)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yNA3_RAsdl0"
   },
   "source": [
    "이는 첫 번째가 \"그리디\"인 이전과 같은 이전 예시와 같은 세 개의 출력 문자열을 생성하여 각 단계에서 로짓의 argmax를 선택합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:15.932515Z",
     "iopub.status.busy": "2022-12-14T21:25:15.931937Z",
     "iopub.status.idle": "2022-12-14T21:25:17.855606Z",
     "shell.execute_reply": "2022-12-14T21:25:17.854573Z"
    },
    "id": "IGVLpzo13rcA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "Generated text for epoch 0: 기타\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\n",
    "\n",
    "class GenerateText:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.image = None\n",
    "    \n",
    "    def load_image(self, img_path):\n",
    "        img = image.load_img(img_path, target_size=(299, 299))  # 수정된 부분\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        return img_array\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def on_epoch_end(self, epoch):\n",
    "        if self.model is None:\n",
    "            print(\"Error: Model is not set. Please set the model using set_model() method.\")\n",
    "            return\n",
    "        \n",
    "        # 예제 이미지 로드 및 전처리\n",
    "        image_url = 'https://tensorflow.org/images/surf.jpg'\n",
    "        image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
    "        self.image = self.load_image(image_path)\n",
    "        \n",
    "        # 예측 수행\n",
    "        logits = self.model.predict(self.image)\n",
    "        pred_indices = np.argmax(logits, axis=1)\n",
    "        \n",
    "        # 예측된 인덱스를 문자열로 변환\n",
    "        label = self.get_label_from_index(pred_indices[0])\n",
    "        print(f\"Generated text for epoch {epoch}: {label}\")\n",
    "    \n",
    "    def get_label_from_index(self, index):\n",
    "        # 여기서는 간단한 예시로 고정된 문자열을 반환하도록 함\n",
    "        if index == 0:\n",
    "            return \"그리디\"\n",
    "        elif index == 1:\n",
    "            return \"알고리즘\"\n",
    "        elif index == 2:\n",
    "            return \"프로그래밍\"\n",
    "        else:\n",
    "            return \"기타\"\n",
    "\n",
    "# 모델 정의 예시 (InceptionV3을 예로 들어보겠습니다)\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# GenerateText 객체 생성 및 사용 예시\n",
    "g = GenerateText()\n",
    "g.set_model(base_model)\n",
    "g.on_epoch_end(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAxp4KZRKDk9"
   },
   "source": [
    "또한 `callbacks.EarlyStopping`을 사용하여 모델이 과적합을 시작할 때 훈련을 중단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:25:17.860025Z",
     "iopub.status.busy": "2022-12-14T21:25:17.859351Z",
     "iopub.status.idle": "2022-12-14T21:25:17.867913Z",
     "shell.execute_reply": "2022-12-14T21:25:17.867126Z"
    },
    "id": "MjzrwGZp23xx"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    GenerateText(),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:29:58.706708Z",
     "iopub.status.busy": "2022-12-14T21:29:58.706433Z",
     "iopub.status.idle": "2022-12-14T21:29:58.868700Z",
     "shell.execute_reply": "2022-12-14T21:29:58.868036Z"
    },
    "id": "yZQ78b2Kxw-T"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 예시 코드 (history 객체가 정의되었다고 가정)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked_acc\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_masked_acc\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mylim([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmasked_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mmax\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_masked_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]))])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 예시 코드 (history 객체가 정의되었다고 가정)\n",
    "plt.plot(history.history['masked_acc'], label='accuracy')\n",
    "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
    "plt.ylim([0, max(max(history.history['masked_acc']), max(history.history['val_masked_acc']))])\n",
    "plt.xlabel('Epoch #')\n",
    "plt.ylabel('CE/token')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQN1qT7KNqbL"
   },
   "source": [
    "## 어텐션 플롯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9XJaC2b2J23"
   },
   "source": [
    "이제 훈련된 모델을 사용하여 이미지에서 해당 `simple_gen` 메서드를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:29:58.872734Z",
     "iopub.status.busy": "2022-12-14T21:29:58.872153Z",
     "iopub.status.idle": "2022-12-14T21:29:59.662816Z",
     "shell.execute_reply": "2022-12-14T21:29:59.662085Z"
    },
    "id": "1UQPtNTb2eu3"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m result\n",
      "Cell \u001b[1;32mIn[81], line 4\u001b[0m, in \u001b[0;36msimple_gen\u001b[1;34m(self, image, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@Captioner\u001b[39m\u001b[38;5;241m.\u001b[39madd_method\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_gen\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m   initial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   img_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m   tokens \u001b[38;5;241m=\u001b[39m initial \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "result = model.simple_gen(image, temperature=0.0)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NXbmeLGN1bJ"
   },
   "source": [
    "출력을 토큰으로 다시 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:29:59.666557Z",
     "iopub.status.busy": "2022-12-14T21:29:59.666038Z",
     "iopub.status.idle": "2022-12-14T21:29:59.669513Z",
     "shell.execute_reply": "2022-12-14T21:29:59.668884Z"
    },
    "id": "zHKOpm0w5Xto"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m str_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      2\u001b[0m str_tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[END]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "str_tokens = result.split()\n",
    "str_tokens.append('[END]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fE-AjuAV55Qo"
   },
   "source": [
    "각 `DecoderLayers`는 `CrossAttention` 레이어에 대한 어텐션 스코어를 캐싱합니다. 각 어텐션 맵의 형태는 `(batch=1, heads, sequence, image)`입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:29:59.672965Z",
     "iopub.status.busy": "2022-12-14T21:29:59.672327Z",
     "iopub.status.idle": "2022-12-14T21:29:59.676973Z",
     "shell.execute_reply": "2022-12-14T21:29:59.676385Z"
    },
    "id": "XZpyuQvq2q-B"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecoderLayer' object has no attribute 'last_attention_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m attn_maps \u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39mlast_attention_scores \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdecoder_layers]\n\u001b[0;32m      2\u001b[0m [\u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mmap\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attn_maps]\n",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m attn_maps \u001b[38;5;241m=\u001b[39m [\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attention_scores\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mdecoder_layers]\n\u001b[0;32m      2\u001b[0m [\u001b[38;5;28mmap\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mmap\u001b[39m \u001b[38;5;129;01min\u001b[39;00m attn_maps]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DecoderLayer' object has no attribute 'last_attention_scores'"
     ]
    }
   ],
   "source": [
    "attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
    "[map.shape for map in attn_maps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T42ImsWv6oHG"
   },
   "source": [
    "따라서 `image` 축을 `height, width`로 다시 분할하는 한편 `batch` 축을 따라 맵을 스택한 다음 `(batch, heads)` 축에 대해 평균을 냅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:29:59.680061Z",
     "iopub.status.busy": "2022-12-14T21:29:59.679811Z",
     "iopub.status.idle": "2022-12-14T21:29:59.687687Z",
     "shell.execute_reply": "2022-12-14T21:29:59.687081Z"
    },
    "id": "ojwtvnkh6mS-"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attn_maps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m attention_maps \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat(\u001b[43mattn_maps\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m attention_maps \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mreduce(\n\u001b[0;32m      3\u001b[0m     attention_maps,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch heads sequence (height width) -> sequence height width\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,\n\u001b[0;32m      6\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'attn_maps' is not defined"
     ]
    }
   ],
   "source": [
    "attention_maps = tf.concat(attn_maps, axis=0)\n",
    "attention_maps = einops.reduce(\n",
    "    attention_maps,\n",
    "    'batch heads sequence (height width) -> sequence height width',\n",
    "    height=7, width=7,\n",
    "    reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TM7rA3zGpJW"
   },
   "source": [
    "이제 각 시퀀스 예측을 위한 단일 어텐션 맵이 하나 있습니다. 각 맵의 값은 합계가 `1`이어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:29:59.691367Z",
     "iopub.status.busy": "2022-12-14T21:29:59.690800Z",
     "iopub.status.idle": "2022-12-14T21:29:59.698804Z",
     "shell.execute_reply": "2022-12-14T21:29:59.698002Z"
    },
    "id": "ASWmWerGCZp3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention_maps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m einops\u001b[38;5;241m.\u001b[39mreduce(\u001b[43mattention_maps\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence height width -> sequence\u001b[39m\u001b[38;5;124m'\u001b[39m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'attention_maps' is not defined"
     ]
    }
   ],
   "source": [
    "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fv7XYGFUd-U7"
   },
   "source": [
    "따라서 다음은 출력에 대한 각 토큰을 생성하는 동안 모델이 어텐션에 주목하는 곳입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:29:59.702239Z",
     "iopub.status.busy": "2022-12-14T21:29:59.701601Z",
     "iopub.status.idle": "2022-12-14T21:29:59.706902Z",
     "shell.execute_reply": "2022-12-14T21:29:59.706280Z"
    },
    "id": "fD_y7PD6RPGt"
   },
   "outputs": [],
   "source": [
    "def plot_attention_maps(image, str_tokens, attention_map):\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "\n",
    "    len_result = len(str_tokens)\n",
    "    \n",
    "    titles = []\n",
    "    for i in range(len_result):\n",
    "      map = attention_map[i]\n",
    "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "      ax = fig.add_subplot(3, grid_size, i+1)\n",
    "      titles.append(ax.set_title(str_tokens[i]))\n",
    "      img = ax.imshow(image)\n",
    "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
    "                clim=[0.0, np.max(map)])\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:29:59.710227Z",
     "iopub.status.busy": "2022-12-14T21:29:59.709662Z",
     "iopub.status.idle": "2022-12-14T21:30:01.515376Z",
     "shell.execute_reply": "2022-12-14T21:30:01.514660Z"
    },
    "id": "PI4NAAws9rvY"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'module' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plot_attention_maps(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m, str_tokens, attention_maps)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'module' and 'int'"
     ]
    }
   ],
   "source": [
    "plot_attention_maps(image/255, str_tokens, attention_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riTz0abQKMkV"
   },
   "source": [
    "이제 더 유용한 함수로 함께 다음과 같이 통합합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:30:01.526459Z",
     "iopub.status.busy": "2022-12-14T21:30:01.525888Z",
     "iopub.status.idle": "2022-12-14T21:30:01.531129Z",
     "shell.execute_reply": "2022-12-14T21:30:01.530496Z"
    },
    "id": "mktpfW-SKQIJ"
   },
   "outputs": [],
   "source": [
    "@Captioner.add_method\n",
    "def run_and_show_attention(self, image, temperature=0.0):\n",
    "  result_txt = self.simple_gen(image, temperature)\n",
    "  str_tokens = result_txt.split()\n",
    "  str_tokens.append('[END]')\n",
    "\n",
    "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
    "  attention_maps = tf.concat(attention_maps, axis=0)\n",
    "  attention_maps = einops.reduce(\n",
    "      attention_maps,\n",
    "      'batch heads sequence (height width) -> sequence height width',\n",
    "      height=7, width=7,\n",
    "      reduction='mean')\n",
    "  \n",
    "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
    "  t = plt.suptitle(result_txt)\n",
    "  t.set_y(1.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:30:01.534419Z",
     "iopub.status.busy": "2022-12-14T21:30:01.533826Z",
     "iopub.status.idle": "2022-12-14T21:30:04.131532Z",
     "shell.execute_reply": "2022-12-14T21:30:04.130789Z"
    },
    "id": "FntRkY11OiMw"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_and_show_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[133], line 3\u001b[0m, in \u001b[0;36mrun_and_show_attention\u001b[1;34m(self, image, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@Captioner\u001b[39m\u001b[38;5;241m.\u001b[39madd_method\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_and_show_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m   result_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   str_tokens \u001b[38;5;241m=\u001b[39m result_txt\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      5\u001b[0m   str_tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[END]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[81], line 4\u001b[0m, in \u001b[0;36msimple_gen\u001b[1;34m(self, image, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@Captioner\u001b[39m\u001b[38;5;241m.\u001b[39madd_method\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_gen\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m   initial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   img_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor(\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m   tokens \u001b[38;5;241m=\u001b[39m initial \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "run_and_show_attention(model, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rprk3HEvZuxb"
   },
   "source": [
    "## 자체 이미지로 시도해보기\n",
    "\n",
    "재미를 위해 방금 훈련한 모델로 자체 이미지를 캡션하는 데 사용할 수 있는 방법을 제공했습니다. 상대적으로 적은 양의 데이터로 훈련되었으므로 이미지가 훈련 데이터와 다를 수 있습니다(결과가 이상할 수 있습니다!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T21:30:04.143623Z",
     "iopub.status.busy": "2022-12-14T21:30:04.142802Z",
     "iopub.status.idle": "2022-12-14T21:30:07.001621Z",
     "shell.execute_reply": "2022-12-14T21:30:07.000849Z"
    },
    "id": "9Psd1quzaAWg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg\n",
      "67460/67460 [==============================] - 0s 3us/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"mobilenetv2_1.00_224\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(1, 1, 224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[135], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m image_path \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_file(origin\u001b[38;5;241m=\u001b[39mimage_url)\n\u001b[0;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(image_path)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mrun_and_show_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[133], line 3\u001b[0m, in \u001b[0;36mrun_and_show_attention\u001b[1;34m(self, image, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@Captioner\u001b[39m\u001b[38;5;241m.\u001b[39madd_method\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_and_show_attention\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m   result_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m   str_tokens \u001b[38;5;241m=\u001b[39m result_txt\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      5\u001b[0m   str_tokens\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[END]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[81], line 4\u001b[0m, in \u001b[0;36msimple_gen\u001b[1;34m(self, image, temperature)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@Captioner\u001b[39m\u001b[38;5;241m.\u001b[39madd_method\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_gen\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m   initial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_to_index([[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m'\u001b[39m]]) \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   img_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m   tokens \u001b[38;5;241m=\u001b[39m initial \u001b[38;5;66;03m# (batch, sequence)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\kiot\\anaconda3\\envs\\bjy8374\\lib\\site-packages\\keras\\engine\\input_spec.py:295\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"mobilenetv2_1.00_224\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(1, 1, 224, 224, 3)"
     ]
    }
   ],
   "source": [
    "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
    "image_path = tf.keras.utils.get_file(origin=image_url)\n",
    "image = load_image(image_path)\n",
    "\n",
    "run_and_show_attention(model, image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "image_captioning.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
